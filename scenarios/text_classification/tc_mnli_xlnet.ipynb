{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) Microsoft Corporation. All rights reserved.*\n",
    "\n",
    "*Licensed under the MIT License.*\n",
    "\n",
    "# Text Classification of MultiNLI Sentences using XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils_nlp.dataset.multinli import load_pandas_df\n",
    "from utils_nlp.eval.classification import eval_classification\n",
    "from utils_nlp.common.timer import Timer\n",
    "from utils_nlp.models.xlnet.common import Language, Tokenizer\n",
    "from utils_nlp.models.xlnet.sequence_classification import XLNetSequenceClassifier\n",
    "from utils_nlp.models.xlnet.utils import generate_confusion_matrix\n",
    "from utils_nlp.models.xlnet.common import log_xlnet_params\n",
    "import mlflow\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook, we fine-tune and evaluate a pretrained [XLNet](https://arxiv.org/abs/1906.08237) model on a subset of the [MultiNLI](https://www.nyu.edu/projects/bowman/multinli/) dataset.\n",
    "\n",
    "We use a [sequence classifier](../../utils_nlp/xlnet/sequence_classification.py) that wraps [Hugging Face's PyTorch implementation](https://github.com/huggingface/pytorch-transformers) of CMU and Google's [XLNet](https://github.com/zihangdai/xlnet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"../../../temp\"\n",
    "XLNET_CACHE_DIR=\"../../../temp\"\n",
    "LANGUAGE = Language.ENGLISHCASED\n",
    "MAX_SEQ_LENGTH = 128\n",
    "BATCH_SIZE = 4\n",
    "NUM_GPUS = 1\n",
    "NUM_EPOCHS = 3\n",
    "TRAIN_SIZE = 0.6\n",
    "LABEL_COL = \"genre\"\n",
    "TEXT_COL = \"sentence1\"\n",
    "WEIGHT_DECAY = 0.0\n",
    "WARMUP_STEPS = 0\n",
    "\n",
    "### Hyperparamters to tune\n",
    "MAX_SEQ_LENGTH = 128\n",
    "LEARNING_RATE = 5e-5\n",
    "ADAM_EPSILON = 1e-8\n",
    "\n",
    "DEBUG = True\n",
    "LOGGING_STEPS = 10\n",
    "mlflow.start_run(run_name = datetime.datetime.now())\n",
    "log_xlnet_params(locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset\n",
    "We start by loading a subset of the data. The following function also downloads and extracts the files, if they don't exist in the data folder.\n",
    "\n",
    "The MultiNLI dataset is mainly used for natural language inference (NLI) tasks, where the inputs are sentence pairs and the labels are entailment indicators. The sentence pairs are also classified into *genres* that allow for more coverage and better evaluation of NLI models.\n",
    "\n",
    "For our classification task, we use the first sentence only as the text input, and the corresponding genre as the label. We select the examples corresponding to one of the entailment labels (*neutral* in this case) to avoid duplicate rows, as the sentences are not unique, whereas the sentence pairs are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_pandas_df(DATA_FOLDER, \"train\")\n",
    "df = df[df[\"gold_label\"]==\"neutral\"]  # get unique sentences\n",
    "\n",
    "if DEBUG:\n",
    "    inds = random.sample(range(len(df.index)), 1000)\n",
    "    df = df.iloc[inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_labels</th>\n",
       "      <th>genre</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>pairID</th>\n",
       "      <th>promptID</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence1_binary_parse</th>\n",
       "      <th>sentence1_parse</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>sentence2_binary_parse</th>\n",
       "      <th>sentence2_parse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286510</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>fiction</td>\n",
       "      <td>neutral</td>\n",
       "      <td>87443n</td>\n",
       "      <td>87443</td>\n",
       "      <td>\"We'll have a delivery of remounts to make to...</td>\n",
       "      <td>( `` ( We ( ( 'll ( have ( ( ( a delivery ) ( ...</td>\n",
       "      <td>(ROOT (S (`` ``) (NP (PRP We)) (VP (MD 'll) (V...</td>\n",
       "      <td>The remounts were ordered by those of the camp.</td>\n",
       "      <td>( ( The remounts ) ( ( were ( ordered ( by ( t...</td>\n",
       "      <td>(ROOT (S (NP (DT The) (NNS remounts)) (VP (VBD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254119</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>government</td>\n",
       "      <td>neutral</td>\n",
       "      <td>143239n</td>\n",
       "      <td>143239</td>\n",
       "      <td>(iii) The regulations under clause (i) may inc...</td>\n",
       "      <td>( ( ( ( -LRB- ( iii -RRB- ) ) ( ( The regulati...</td>\n",
       "      <td>(ROOT (S (NP (NP (NP (-LRB- -LRB-) (NN iii) (-...</td>\n",
       "      <td>Emissions reductions are actively being pursued.</td>\n",
       "      <td>( ( Emissions reductions ) ( ( ( are actively ...</td>\n",
       "      <td>(ROOT (S (NP (JJ Emissions) (NNS reductions)) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232736</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>government</td>\n",
       "      <td>neutral</td>\n",
       "      <td>91574n</td>\n",
       "      <td>91574</td>\n",
       "      <td>America needs a clean, secure, affordable, rel...</td>\n",
       "      <td>( America ( ( ( needs ( ( a ( ( clean ( , ( se...</td>\n",
       "      <td>(ROOT (S (NP (NNP America)) (VP (VBZ needs) (N...</td>\n",
       "      <td>Many believe that without changes to the energ...</td>\n",
       "      <td>( Many ( ( believe ( that ( ( without ( change...</td>\n",
       "      <td>(ROOT (S (NP (DT Many)) (VP (VBP believe) (SBA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281705</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>slate</td>\n",
       "      <td>neutral</td>\n",
       "      <td>100430n</td>\n",
       "      <td>100430</td>\n",
       "      <td>(Impeachment, of course, is hardly the sole ca...</td>\n",
       "      <td>( -LRB- ( ( ( ( Impeachment , ) ( of course ) ...</td>\n",
       "      <td>(ROOT (S (-LRB- -LRB-) (NP (NP (NNP Impeachmen...</td>\n",
       "      <td>He was paralyzed after he fell down some stairs.</td>\n",
       "      <td>( He ( ( was ( paralyzed ( after ( he ( ( fell...</td>\n",
       "      <td>(ROOT (S (NP (PRP He)) (VP (VBD was) (VP (VBN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344823</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>travel</td>\n",
       "      <td>neutral</td>\n",
       "      <td>142225n</td>\n",
       "      <td>142225</td>\n",
       "      <td>Next door on the busy pedestrian street of Cor...</td>\n",
       "      <td>( ( ( Next door ) ( on ( ( the ( busy ( pedest...</td>\n",
       "      <td>(ROOT (S (NP (NP (JJ Next) (NN door)) (PP (IN ...</td>\n",
       "      <td>The 15-century hall and chapel is open for tou...</td>\n",
       "      <td>( ( ( ( The ( 15-century hall ) ) and ) chapel...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT The) (JJ 15-century) (NN ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       annotator_labels       genre gold_label   pairID  promptID  \\\n",
       "286510        [neutral]     fiction    neutral   87443n     87443   \n",
       "254119        [neutral]  government    neutral  143239n    143239   \n",
       "232736        [neutral]  government    neutral   91574n     91574   \n",
       "281705        [neutral]       slate    neutral  100430n    100430   \n",
       "344823        [neutral]      travel    neutral  142225n    142225   \n",
       "\n",
       "                                                sentence1  \\\n",
       "286510   \"We'll have a delivery of remounts to make to...   \n",
       "254119  (iii) The regulations under clause (i) may inc...   \n",
       "232736  America needs a clean, secure, affordable, rel...   \n",
       "281705  (Impeachment, of course, is hardly the sole ca...   \n",
       "344823  Next door on the busy pedestrian street of Cor...   \n",
       "\n",
       "                                   sentence1_binary_parse  \\\n",
       "286510  ( `` ( We ( ( 'll ( have ( ( ( a delivery ) ( ...   \n",
       "254119  ( ( ( ( -LRB- ( iii -RRB- ) ) ( ( The regulati...   \n",
       "232736  ( America ( ( ( needs ( ( a ( ( clean ( , ( se...   \n",
       "281705  ( -LRB- ( ( ( ( Impeachment , ) ( of course ) ...   \n",
       "344823  ( ( ( Next door ) ( on ( ( the ( busy ( pedest...   \n",
       "\n",
       "                                          sentence1_parse  \\\n",
       "286510  (ROOT (S (`` ``) (NP (PRP We)) (VP (MD 'll) (V...   \n",
       "254119  (ROOT (S (NP (NP (NP (-LRB- -LRB-) (NN iii) (-...   \n",
       "232736  (ROOT (S (NP (NNP America)) (VP (VBZ needs) (N...   \n",
       "281705  (ROOT (S (-LRB- -LRB-) (NP (NP (NNP Impeachmen...   \n",
       "344823  (ROOT (S (NP (NP (JJ Next) (NN door)) (PP (IN ...   \n",
       "\n",
       "                                                sentence2  \\\n",
       "286510   The remounts were ordered by those of the camp.    \n",
       "254119   Emissions reductions are actively being pursued.   \n",
       "232736  Many believe that without changes to the energ...   \n",
       "281705   He was paralyzed after he fell down some stairs.   \n",
       "344823  The 15-century hall and chapel is open for tou...   \n",
       "\n",
       "                                   sentence2_binary_parse  \\\n",
       "286510  ( ( The remounts ) ( ( were ( ordered ( by ( t...   \n",
       "254119  ( ( Emissions reductions ) ( ( ( are actively ...   \n",
       "232736  ( Many ( ( believe ( that ( ( without ( change...   \n",
       "281705  ( He ( ( was ( paralyzed ( after ( he ( ( fell...   \n",
       "344823  ( ( ( ( The ( 15-century hall ) ) and ) chapel...   \n",
       "\n",
       "                                          sentence2_parse  \n",
       "286510  (ROOT (S (NP (DT The) (NNS remounts)) (VP (VBD...  \n",
       "254119  (ROOT (S (NP (JJ Emissions) (NNS reductions)) ...  \n",
       "232736  (ROOT (S (NP (DT Many)) (VP (VBP believe) (SBA...  \n",
       "281705  (ROOT (S (NP (PRP He)) (VP (VBD was) (VP (VBN ...  \n",
       "344823  (ROOT (S (NP (NP (DT The) (JJ 15-century) (NN ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples in the dataset are grouped into 5 genres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "government    214\n",
       "fiction       211\n",
       "telephone     201\n",
       "travel        200\n",
       "slate         174\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[LABEL_COL].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data for training and testing, and encode the class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split\n",
    "df_train, df_test = train_test_split(df, train_size = TRAIN_SIZE)\n",
    "\n",
    "# encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_train = label_encoder.fit_transform(df_train[LABEL_COL])\n",
    "labels_test = label_encoder.transform(df_test[LABEL_COL])\n",
    "label_list = label_encoder.classes_\n",
    "\n",
    "num_labels = len(np.unique(labels_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 5\n",
      "Number of training examples: 600\n",
      "Number of testing examples: 400\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique labels: {}\".format(num_labels))\n",
    "print(\"Number of training examples: {}\".format(df_train.shape[0]))\n",
    "print(\"Number of testing examples: {}\".format(df_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Preprocess\n",
    "Before training, we tokenize the text documents and convert them to lists of tokens. The following steps instantiate a XLNet tokenizer given the language, and tokenize the text of the training and testing sets.\n",
    "\n",
    "We perform the following preprocessing steps in the cell below:\n",
    "- Convert the tokens into token indices corresponding to the XLNet-base tokenizer's vocabulary\n",
    "- Add the special tokens [CLS] and [SEP] to mark the end of a sentence\n",
    "- Pad or truncate the token lists to the specified max length\n",
    "- Return id lists that indicate which word the tokens map to\n",
    "- Return mask lists that indicate paddings' positions\n",
    "- Return segment type id lists that indicates which segment each the tokens belongs to\n",
    "\n",
    "*See the pytorch-transformer [implementation](https://github.com/huggingface/pytorch-transformers/blob/master/examples/utils_glue.py) for more information on XLNet's input format.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(LANGUAGE, cache_dir=XLNET_CACHE_DIR)\n",
    "\n",
    "train_input_ids, train_input_mask, train_segment_ids = tokenizer.preprocess_classification_tokens(list(df_train[TEXT_COL]), MAX_SEQ_LENGTH)\n",
    "test_input_ids, test_input_mask, test_segment_ids = tokenizer.preprocess_classification_tokens(list(df_test[TEXT_COL]), MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "Next, we create a sequence classifier that loads a pre-trained XLNet model, given the language and number of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = XLNetSequenceClassifier(\n",
    "    language=LANGUAGE,\n",
    "    num_labels=num_labels,\n",
    "    cache_dir=XLNET_CACHE_DIR,\n",
    "    num_gpus=NUM_GPUS,        \n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "We train the classifier using the training examples. This involves fine-tuning the XLNet Transformer and learning a linear classification layer on top of that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   1%|          | 1/150 [00:00<01:22,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/3; batch:1->16/150; average training loss:1.727106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  11%|█▏        | 17/150 [00:06<00:51,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/3; batch:17->32/150; average training loss:1.785313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  22%|██▏       | 33/150 [00:13<00:45,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/3; batch:33->48/150; average training loss:1.698002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  33%|███▎      | 49/150 [00:19<00:39,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/3; batch:49->64/150; average training loss:1.571066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  34%|███▍      | 51/150 [00:20<00:38,  2.57it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-244852076861>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mlogging_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0msave_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     )    \n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[Training time: {:.3f} hrs]\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterval\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m3600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/anpimple/notebooks/nlp/utils_nlp/models/xlnet/sequence_classification.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, token_ids, input_mask, labels, token_type_ids, verbose, logging_steps, save_steps, output_dir)\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                     x_batch, mask_batch, token_type_ids_batch, y_batch = tuple(\n\u001b[0;32m--> 177\u001b[0;31m                         \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m                     )\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/anpimple/notebooks/nlp/utils_nlp/models/xlnet/sequence_classification.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                     x_batch, mask_batch, token_type_ids_batch, y_batch = tuple(\n\u001b[0;32m--> 177\u001b[0;31m                         \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m                     )\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with Timer() as t:\n",
    "    classifier.fit(\n",
    "        token_ids=train_input_ids,\n",
    "        input_mask=train_input_mask,\n",
    "        token_type_ids=train_segment_ids,\n",
    "        labels=labels_train,  \n",
    "        verbose=True,\n",
    "        logging_steps = LOGGING_STEPS,\n",
    "    )    \n",
    "print(\"[Training time: {:.3f} hrs]\".format(t.interval / 3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score\n",
    "We score the test set using the trained classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = classifier.predict(\n",
    "    token_ids=test_input_ids,\n",
    "    input_mask=test_input_mask,\n",
    "    token_type_ids=test_segment_ids,\n",
    "    num_gpus=NUM_GPUS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    probabilities=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Results\n",
    "Finally, we compute the accuracy, precision, recall, and F1 metrics of the evaluation on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_report = classification_report(labels_test, preds, target_names=label_encoder.classes_)\n",
    "print(cls_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Confusion Matrix using Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_confusion_matrix(labels_test,preds,label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_gpu_dsvm)",
   "language": "python",
   "name": "nlp_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
