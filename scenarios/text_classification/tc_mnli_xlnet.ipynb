{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) Microsoft Corporation. All rights reserved.*\n",
    "\n",
    "*Licensed under the MIT License.*\n",
    "\n",
    "# Text Classification of MultiNLI Sentences using XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils_nlp.dataset.multinli import load_pandas_df\n",
    "from utils_nlp.eval.classification import eval_classification\n",
    "from utils_nlp.common.timer import Timer\n",
    "from utils_nlp.models.xlnet.common import Language, Tokenizer\n",
    "from utils_nlp.models.xlnet.sequence_classification import XLNetSequenceClassifier\n",
    "from utils_nlp.models.xlnet.utils import generate_confusion_matrix\n",
    "from utils_nlp.models.xlnet.common import log_xlnet_params\n",
    "import mlflow\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook, we fine-tune and evaluate a pretrained [XLNet](https://arxiv.org/abs/1906.08237) model on a subset of the [MultiNLI](https://www.nyu.edu/projects/bowman/multinli/) dataset.\n",
    "\n",
    "We use a [sequence classifier](../../utils_nlp/xlnet/sequence_classification.py) that wraps [Hugging Face's PyTorch implementation](https://github.com/huggingface/pytorch-transformers) of CMU and Google's [XLNet](https://github.com/zihangdai/xlnet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"../../../temp\"\n",
    "XLNET_CACHE_DIR=\"../../../temp\"\n",
    "LANGUAGE = Language.ENGLISHCASED\n",
    "MAX_SEQ_LENGTH = 128\n",
    "BATCH_SIZE = 4\n",
    "NUM_GPUS = 1\n",
    "NUM_EPOCHS = 1\n",
    "TRAIN_SIZE = 0.6\n",
    "LABEL_COL = \"genre\"\n",
    "TEXT_COL = \"sentence1\"\n",
    "WEIGHT_DECAY = 0.0\n",
    "WARMUP_STEPS = 0\n",
    "\n",
    "### Hyperparamters to tune\n",
    "MAX_SEQ_LENGTH = 128\n",
    "LEARNING_RATE = 5e-5\n",
    "ADAM_EPSILON = 1e-8\n",
    "\n",
    "DEBUG = True\n",
    "LOGGING_STEPS = 10\n",
    "SAVE_STEPS = 10\n",
    "mlflow.start_run(run_name = datetime.datetime.now())\n",
    "log_xlnet_params(locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset\n",
    "We start by loading a subset of the data. The following function also downloads and extracts the files, if they don't exist in the data folder.\n",
    "\n",
    "The MultiNLI dataset is mainly used for natural language inference (NLI) tasks, where the inputs are sentence pairs and the labels are entailment indicators. The sentence pairs are also classified into *genres* that allow for more coverage and better evaluation of NLI models.\n",
    "\n",
    "For our classification task, we use the first sentence only as the text input, and the corresponding genre as the label. We select the examples corresponding to one of the entailment labels (*neutral* in this case) to avoid duplicate rows, as the sentences are not unique, whereas the sentence pairs are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_pandas_df(DATA_FOLDER, \"train\")\n",
    "df = df[df[\"gold_label\"]==\"neutral\"]  # get unique sentences\n",
    "\n",
    "if DEBUG:\n",
    "    inds = random.sample(range(len(df.index)), 1000)\n",
    "    df = df.iloc[inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_labels</th>\n",
       "      <th>genre</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>pairID</th>\n",
       "      <th>promptID</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence1_binary_parse</th>\n",
       "      <th>sentence1_parse</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>sentence2_binary_parse</th>\n",
       "      <th>sentence2_parse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>155920</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>slate</td>\n",
       "      <td>neutral</td>\n",
       "      <td>124353n</td>\n",
       "      <td>124353</td>\n",
       "      <td>For my part, I plan to endorse the Baptist boy...</td>\n",
       "      <td>( ( For ( my part ) ) ( , ( I ( ( plan ( to ( ...</td>\n",
       "      <td>(ROOT (S (PP (IN For) (NP (PRP$ my) (NN part))...</td>\n",
       "      <td>Boycotting will send a message.</td>\n",
       "      <td>( Boycotting ( ( will ( send ( a message ) ) )...</td>\n",
       "      <td>(ROOT (S (NP (NNP Boycotting)) (VP (MD will) (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211456</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>government</td>\n",
       "      <td>neutral</td>\n",
       "      <td>106327n</td>\n",
       "      <td>106327</td>\n",
       "      <td>CSIS Policy Summit on Global Aging, Washington...</td>\n",
       "      <td>( ( CSIS Policy ) ( ( ( Summit ( on ( Global (...</td>\n",
       "      <td>(ROOT (S (NP (NNP CSIS) (NNP Policy)) (VP (VBD...</td>\n",
       "      <td>More than 20 guest speakers attended this summit.</td>\n",
       "      <td>( ( ( More ( than 20 ) ) ( guest speakers ) ) ...</td>\n",
       "      <td>(ROOT (S (NP (QP (JJR More) (IN than) (CD 20))...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293188</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>slate</td>\n",
       "      <td>neutral</td>\n",
       "      <td>71440n</td>\n",
       "      <td>71440</td>\n",
       "      <td>The beers used in the experiment were as</td>\n",
       "      <td>( ( ( The beers ) ( used ( in ( the experiment...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT The) (NNS beers)) (VP (VB...</td>\n",
       "      <td>The beers were part of the experiment conducte...</td>\n",
       "      <td>( ( The beers ) ( ( were ( part ( of ( ( the e...</td>\n",
       "      <td>(ROOT (S (NP (DT The) (NNS beers)) (VP (VBD we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275458</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>government</td>\n",
       "      <td>neutral</td>\n",
       "      <td>15842n</td>\n",
       "      <td>15842</td>\n",
       "      <td>Postal Service and the given post.</td>\n",
       "      <td>( ( ( ( Postal Service ) and ) ( the ( given p...</td>\n",
       "      <td>(ROOT (NP (NP (NP (NNP Postal) (NNP Service)) ...</td>\n",
       "      <td>the postal services is at an address</td>\n",
       "      <td>( ( the ( postal services ) ) ( is ( at ( an a...</td>\n",
       "      <td>(ROOT (S (NP (DT the) (JJ postal) (NNS service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85808</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>telephone</td>\n",
       "      <td>neutral</td>\n",
       "      <td>83172n</td>\n",
       "      <td>83172</td>\n",
       "      <td>and the new place doesn't have air conditionin...</td>\n",
       "      <td>( and ( ( the ( new place ) ) ( ( ( ( does n't...</td>\n",
       "      <td>(ROOT (FRAG (CC and) (NP (NP (DT the) (JJ new)...</td>\n",
       "      <td>Air conditioning costs a lot of money so not h...</td>\n",
       "      <td>( ( Air conditioning ) ( ( ( costs ( ( a lot )...</td>\n",
       "      <td>(ROOT (S (NP (NNP Air) (NNP conditioning)) (VP...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       annotator_labels       genre gold_label   pairID  promptID  \\\n",
       "155920        [neutral]       slate    neutral  124353n    124353   \n",
       "211456        [neutral]  government    neutral  106327n    106327   \n",
       "293188        [neutral]       slate    neutral   71440n     71440   \n",
       "275458        [neutral]  government    neutral   15842n     15842   \n",
       "85808         [neutral]   telephone    neutral   83172n     83172   \n",
       "\n",
       "                                                sentence1  \\\n",
       "155920  For my part, I plan to endorse the Baptist boy...   \n",
       "211456  CSIS Policy Summit on Global Aging, Washington...   \n",
       "293188          The beers used in the experiment were as    \n",
       "275458                 Postal Service and the given post.   \n",
       "85808   and the new place doesn't have air conditionin...   \n",
       "\n",
       "                                   sentence1_binary_parse  \\\n",
       "155920  ( ( For ( my part ) ) ( , ( I ( ( plan ( to ( ...   \n",
       "211456  ( ( CSIS Policy ) ( ( ( Summit ( on ( Global (...   \n",
       "293188  ( ( ( The beers ) ( used ( in ( the experiment...   \n",
       "275458  ( ( ( ( Postal Service ) and ) ( the ( given p...   \n",
       "85808   ( and ( ( the ( new place ) ) ( ( ( ( does n't...   \n",
       "\n",
       "                                          sentence1_parse  \\\n",
       "155920  (ROOT (S (PP (IN For) (NP (PRP$ my) (NN part))...   \n",
       "211456  (ROOT (S (NP (NNP CSIS) (NNP Policy)) (VP (VBD...   \n",
       "293188  (ROOT (S (NP (NP (DT The) (NNS beers)) (VP (VB...   \n",
       "275458  (ROOT (NP (NP (NP (NNP Postal) (NNP Service)) ...   \n",
       "85808   (ROOT (FRAG (CC and) (NP (NP (DT the) (JJ new)...   \n",
       "\n",
       "                                                sentence2  \\\n",
       "155920                    Boycotting will send a message.   \n",
       "211456  More than 20 guest speakers attended this summit.   \n",
       "293188  The beers were part of the experiment conducte...   \n",
       "275458               the postal services is at an address   \n",
       "85808   Air conditioning costs a lot of money so not h...   \n",
       "\n",
       "                                   sentence2_binary_parse  \\\n",
       "155920  ( Boycotting ( ( will ( send ( a message ) ) )...   \n",
       "211456  ( ( ( More ( than 20 ) ) ( guest speakers ) ) ...   \n",
       "293188  ( ( The beers ) ( ( were ( part ( of ( ( the e...   \n",
       "275458  ( ( the ( postal services ) ) ( is ( at ( an a...   \n",
       "85808   ( ( Air conditioning ) ( ( ( costs ( ( a lot )...   \n",
       "\n",
       "                                          sentence2_parse  \n",
       "155920  (ROOT (S (NP (NNP Boycotting)) (VP (MD will) (...  \n",
       "211456  (ROOT (S (NP (QP (JJR More) (IN than) (CD 20))...  \n",
       "293188  (ROOT (S (NP (DT The) (NNS beers)) (VP (VBD we...  \n",
       "275458  (ROOT (S (NP (DT the) (JJ postal) (NNS service...  \n",
       "85808   (ROOT (S (NP (NNP Air) (NNP conditioning)) (VP...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples in the dataset are grouped into 5 genres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "slate         223\n",
       "government    206\n",
       "telephone     202\n",
       "travel        191\n",
       "fiction       178\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[LABEL_COL].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data for training and testing, and encode the class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split\n",
    "df_train, df_test = train_test_split(df, train_size = TRAIN_SIZE)\n",
    "\n",
    "# encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_train = label_encoder.fit_transform(df_train[LABEL_COL])\n",
    "labels_test = label_encoder.transform(df_test[LABEL_COL])\n",
    "label_list = label_encoder.classes_\n",
    "\n",
    "num_labels = len(np.unique(labels_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 5\n",
      "Number of training examples: 600\n",
      "Number of testing examples: 400\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique labels: {}\".format(num_labels))\n",
    "print(\"Number of training examples: {}\".format(df_train.shape[0]))\n",
    "print(\"Number of testing examples: {}\".format(df_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Preprocess\n",
    "Before training, we tokenize the text documents and convert them to lists of tokens. The following steps instantiate a XLNet tokenizer given the language, and tokenize the text of the training and testing sets.\n",
    "\n",
    "We perform the following preprocessing steps in the cell below:\n",
    "- Convert the tokens into token indices corresponding to the XLNet-base tokenizer's vocabulary\n",
    "- Add the special tokens [CLS] and [SEP] to mark the end of a sentence\n",
    "- Pad or truncate the token lists to the specified max length\n",
    "- Return id lists that indicate which word the tokens map to\n",
    "- Return mask lists that indicate paddings' positions\n",
    "- Return segment type id lists that indicates which segment each the tokens belongs to\n",
    "\n",
    "*See the pytorch-transformer [implementation](https://github.com/huggingface/pytorch-transformers/blob/master/examples/utils_glue.py) for more information on XLNet's input format.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(LANGUAGE, cache_dir=XLNET_CACHE_DIR)\n",
    "\n",
    "train_input_ids, train_input_mask, train_segment_ids = tokenizer.preprocess_classification_tokens(list(df_train[TEXT_COL]), MAX_SEQ_LENGTH)\n",
    "test_input_ids, test_input_mask, test_segment_ids = tokenizer.preprocess_classification_tokens(list(df_test[TEXT_COL]), MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "Next, we create a sequence classifier that loads a pre-trained XLNet model, given the language and number of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = XLNetSequenceClassifier(\n",
    "    language=LANGUAGE,\n",
    "    num_labels=num_labels,\n",
    "    cache_dir=XLNET_CACHE_DIR,\n",
    "    num_gpus=NUM_GPUS,        \n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "We train the classifier using the training examples. This involves fine-tuning the XLNet Transformer and learning a linear classification layer on top of that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   1%|          | 1/150 [00:00<01:09,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/1; batch:1->16/150; average training loss:1.566105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  11%|█▏        | 17/150 [00:11<01:07,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/1; batch:17->32/150; average training loss:1.768401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  22%|██▏       | 33/150 [00:24<01:53,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/1; batch:33->48/150; average training loss:1.676888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  33%|███▎      | 49/150 [00:32<00:42,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/1; batch:49->64/150; average training loss:1.614675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  43%|████▎     | 65/150 [00:53<01:16,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/1; batch:65->80/150; average training loss:1.513738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  54%|█████▍    | 81/150 [01:18<01:57,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/1; batch:81->96/150; average training loss:1.464170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  65%|██████▍   | 97/150 [01:25<00:22,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/1; batch:97->112/150; average training loss:1.415893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  66%|██████▌   | 99/150 [01:26<00:20,  2.47it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4eecf5140e13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mlogging_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLOGGING_STEPS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0msave_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSAVE_STEPS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     )    \n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[Training time: {:.3f} hrs]\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterval\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m3600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/anpimple/notebooks/nlp/utils_nlp/models/xlnet/sequence_classification.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, token_ids, input_mask, labels, token_type_ids, verbose, logging_steps, save_steps)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# model outputs are always tuple in pytorch-transformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/nlp_gpu/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/nlp_gpu/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with Timer() as t:\n",
    "    classifier.fit(\n",
    "        token_ids=train_input_ids,\n",
    "        input_mask=train_input_mask,\n",
    "        token_type_ids=train_segment_ids,\n",
    "        labels=labels_train,  \n",
    "        verbose=True,\n",
    "        logging_steps = LOGGING_STEPS,\n",
    "        save_steps = SAVE_STEPS,\n",
    "    )    \n",
    "print(\"[Training time: {:.3f} hrs]\".format(t.interval / 3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score\n",
    "We score the test set using the trained classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = classifier.predict(\n",
    "    token_ids=test_input_ids,\n",
    "    input_mask=test_input_mask,\n",
    "    token_type_ids=test_segment_ids,\n",
    "    num_gpus=NUM_GPUS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    probabilities=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Results\n",
    "Finally, we compute the accuracy, precision, recall, and F1 metrics of the evaluation on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_report = classification_report(labels_test, preds, target_names=label_encoder.classes_,output_dict=True)\n",
    "print(classification_report(labels_test, preds, target_names=label_encoder.classes_))\n",
    "\n",
    "cls_report_df = pd.DataFrame(cls_report)\n",
    "cls_report_df.to_csv(path_or_buf=os.path.join(os.getcwd(),\"checkpoints\",\"cls_report.csv\"))\n",
    "mlflow.log_artifact(os.path.join(os.getcwd(),\"checkpoints\",\"cls_report.csv\"))\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Confusion Matrix using Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_confusion_matrix(labels_test,preds,label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_gpu_dsvm)",
   "language": "python",
   "name": "nlp_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
