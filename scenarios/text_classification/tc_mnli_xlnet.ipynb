{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "\n",
    "# This script reuses some code from\n",
    "# https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples\n",
    "# /run_classifier.py\n",
    "\n",
    "\n",
    "from enum import Enum\n",
    "import warnings\n",
    "from collections import Iterable\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from pytorch_transformers import (WEIGHTS_NAME, XLNetConfig,XLNetForSequenceClassification,XLNetTokenizer)\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "\n",
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    "    RandomSampler,\n",
    "    SequentialSampler,\n",
    "    TensorDataset,\n",
    ")\n",
    "\n",
    "# Max supported sequence length\n",
    "BERT_MAX_LEN = 512\n",
    "\n",
    "\n",
    "class Language(Enum):\n",
    "    \"\"\"An enumeration of the supported pretrained models and languages.\"\"\"\n",
    "\n",
    "    ENGLISH = \"bert-base-uncased\"\n",
    "    ENGLISHCASED = \"bert-base-cased\"\n",
    "    ENGLISHLARGE = \"bert-large-uncased\"\n",
    "    ENGLISHLARGECASED = \"bert-large-cased\"\n",
    "    ENGLISHLARGEWWM = \"bert-large-uncased-whole-word-masking\"\n",
    "    ENGLISHLARGECASEDWWM = \"bert-large-cased-whole-word-masking\"\n",
    "    CHINESE = \"bert-base-chinese\"\n",
    "    MULTILINGUAL = \"bert-base-multilingual-cased\"\n",
    "\n",
    "\n",
    "class XLTokenizer:\n",
    "    def __init__(\n",
    "        self, language=Language.ENGLISH, to_lower=False, cache_dir=\".\"\n",
    "    ):\n",
    "        \"\"\"Initializes the underlying pretrained BERT tokenizer.\n",
    "\n",
    "        Args:\n",
    "            language (Language, optional): The pretrained model's language.\n",
    "                                           Defaults to Language.ENGLISH.\n",
    "            cache_dir (str, optional): Location of BERT's cache directory.\n",
    "                Defaults to \".\".\n",
    "        \"\"\"\n",
    "        self.tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "        self.language = language\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenizes a list of documents using a BERT tokenizer\n",
    "\n",
    "        Args:\n",
    "            text (list): List of strings (one sequence) or\n",
    "                tuples (two sequences).\n",
    "\n",
    "        Returns:\n",
    "            [list]: List of lists. Each sublist contains WordPiece tokens\n",
    "                of the input sequence(s).\n",
    "        \"\"\"\n",
    "        if isinstance(text[0], str):\n",
    "            return [self.tokenizer.encode(x) for x in text]\n",
    "        else:\n",
    "            return [\n",
    "                [self.tokenizer.encode(x) for x in sentences]\n",
    "                for sentences in text\n",
    "            ]\n",
    "\n",
    "    def _truncate_seq_pair(self, tokens_a, tokens_b, max_length):\n",
    "        \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "        # This is a simple heuristic which will always truncate the longer\n",
    "        # sequence one token at a time. This makes more sense than\n",
    "        # truncating an equal percent of tokens from each, since if one\n",
    "        # sequence is very short then each token that's truncated likely\n",
    "        # contains more information than a longer sequence.\n",
    "        while True:\n",
    "            total_length = len(tokens_a) + len(tokens_b)\n",
    "            if total_length <= max_length:\n",
    "                break\n",
    "            if len(tokens_a) > len(tokens_b):\n",
    "                tokens_a.pop()\n",
    "            else:\n",
    "                tokens_b.pop()\n",
    "\n",
    "        tokens_a.append(\"[SEP]\")\n",
    "        tokens_b.append(\"[SEP]\")\n",
    "\n",
    "        return [tokens_a, tokens_b]\n",
    "\n",
    "    def preprocess_classification_tokens(self, tokens, max_len=BERT_MAX_LEN):\n",
    "        \"\"\"Preprocessing of input tokens:\n",
    "            - add BERT sentence markers ([CLS] and [SEP])\n",
    "            - map tokens to token indices in the BERT vocabulary\n",
    "            - pad and truncate sequences\n",
    "            - create an input_mask\n",
    "            - create token type ids, aka. segment ids\n",
    "\n",
    "        Args:\n",
    "            tokens (list): List of token lists to preprocess.\n",
    "            max_len (int, optional): Maximum number of tokens\n",
    "                            (documents will be truncated or padded).\n",
    "                            Defaults to 512.\n",
    "        Returns:\n",
    "            tuple: A tuple containing the following three lists\n",
    "                list of preprocesssed token lists\n",
    "                list of input mask lists\n",
    "                list of token type id lists\n",
    "        \"\"\"\n",
    "        if max_len > BERT_MAX_LEN:\n",
    "            print(\n",
    "                \"setting max_len to max allowed tokens: {}\".format(\n",
    "                    BERT_MAX_LEN\n",
    "                )\n",
    "            )\n",
    "            max_len = BERT_MAX_LEN\n",
    "\n",
    "        if isinstance(tokens[0][0], str):\n",
    "            tokens = [x[0 : max_len - 2] + [\"[SEP]\"] for x in tokens]\n",
    "            token_type_ids = None\n",
    "        else:\n",
    "            # get tokens for each sentence [[t00, t01, ...] [t10, t11,... ]]\n",
    "            tokens = [\n",
    "                self._truncate_seq_pair(sentence[0], sentence[1], max_len - 3)\n",
    "                for sentence in tokens\n",
    "            ]\n",
    "\n",
    "            # construct token_type_ids\n",
    "            # [[0, 0, 0, 0, ... 0, 1, 1, 1, ... 1], [0, 0, 0, ..., 1, 1, ]\n",
    "            token_type_ids = [\n",
    "                [[i] * len(sentence) for i, sentence in enumerate(example)]\n",
    "                for example in tokens\n",
    "            ]\n",
    "            # merge sentences\n",
    "            tokens = [\n",
    "                [token for sentence in example for token in sentence]\n",
    "                for example in tokens\n",
    "            ]\n",
    "            # prefix with [0] for [CLS]\n",
    "            token_type_ids = [\n",
    "                [0] + [i for sentence in example for i in sentence]\n",
    "                for example in token_type_ids\n",
    "            ]\n",
    "            # pad sequence\n",
    "            token_type_ids = [\n",
    "                x + [0] * (max_len - len(x)) for x in token_type_ids\n",
    "            ]\n",
    "\n",
    "        tokens = [[\"[CLS]\"] + x for x in tokens]\n",
    "        # convert tokens to indices\n",
    "        tokens = [self.tokenizer.convert_tokens_to_ids(x) for x in tokens]\n",
    "        # pad sequence\n",
    "        tokens = [x + [0] * (max_len - len(x)) for x in tokens]\n",
    "        # create input mask\n",
    "        input_mask = [[min(1, x) for x in y] for y in tokens]\n",
    "        return tokens, input_mask, token_type_ids\n",
    "\n",
    "    def tokenize_ner(\n",
    "        self,\n",
    "        text,\n",
    "        max_len=BERT_MAX_LEN,\n",
    "        labels=None,\n",
    "        label_map=None,\n",
    "        trailing_piece_tag=\"X\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Tokenize and preprocesses input word lists, involving the following steps\n",
    "            0. WordPiece tokenization.\n",
    "            1. Convert string tokens to token ids.\n",
    "            2. Convert input labels to label ids, if labels and label_map are\n",
    "                provided.\n",
    "            3. If a word is tokenized into multiple pieces of tokens by the\n",
    "                WordPiece tokenizer, label the extra tokens with\n",
    "                trailing_piece_tag.\n",
    "            4. Pad or truncate input text according to max_seq_length\n",
    "            5. Create input_mask for masking out padded tokens.\n",
    "\n",
    "        Args:\n",
    "            text (list): List of lists. Each sublist is a list of words in an\n",
    "                input sentence.\n",
    "            max_len (int, optional): Maximum length of the list of\n",
    "                tokens. Lists longer than this are truncated and shorter\n",
    "                ones are padded with \"O\"s. Default value is BERT_MAX_LEN=512.\n",
    "            labels (list, optional): List of word label lists. Each sublist\n",
    "                contains labels corresponding to the input word list. The lengths\n",
    "                of the label list and word list must be the same. Default\n",
    "                value is None.\n",
    "            label_map (dict, optional): Dictionary for mapping original token\n",
    "                labels (which may be string type) to integers. Default value\n",
    "                is None.\n",
    "            trailing_piece_tag (str, optional): Tag used to label trailing\n",
    "                word pieces. For example, \"criticize\" is broken into \"critic\"\n",
    "                and \"##ize\", \"critic\" preserves its original label and \"##ize\"\n",
    "                is labeled as trailing_piece_tag. Default value is \"X\".\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the following four lists.\n",
    "                1. input_ids_all: List of lists. Each sublist contains\n",
    "                    numerical values, i.e. token ids, corresponding to the\n",
    "                    tokens in the input text data.\n",
    "                2. input_mask_all: List of lists. Each sublist\n",
    "                    contains the attention mask of the input token id list,\n",
    "                    1 for input tokens and 0 for padded tokens, so that\n",
    "                    padded tokens are not attended to.\n",
    "                3. trailing_token_mask: List of lists. Each sublist is\n",
    "                    a boolean list, True for the first word piece of each\n",
    "                    original word, False for the trailing word pieces,\n",
    "                    e.g. \"##ize\". This mask is useful for removing the\n",
    "                    predictions on trailing word pieces, so that each\n",
    "                    original word in the input text has a unique predicted\n",
    "                    label.\n",
    "                4. label_ids_all: List of lists of numerical labels,\n",
    "                    each sublist contains token labels of a input\n",
    "                    sentence/paragraph, if labels is provided. If the `labels`\n",
    "                    argument is not provided, the value of this is None.\n",
    "        \"\"\"\n",
    "\n",
    "        def _is_iterable_but_not_string(obj):\n",
    "            return isinstance(obj, Iterable) and not isinstance(obj, str)\n",
    "\n",
    "        if max_len > BERT_MAX_LEN:\n",
    "            warnings.warn(\n",
    "                \"setting max_len to max allowed tokens: {}\".format(\n",
    "                    BERT_MAX_LEN\n",
    "                )\n",
    "            )\n",
    "            max_len = BERT_MAX_LEN\n",
    "\n",
    "        if not _is_iterable_but_not_string(text):\n",
    "            # The input text must be an non-string Iterable\n",
    "            raise ValueError(\n",
    "                \"Input text must be an iterable and not a string.\"\n",
    "            )\n",
    "        else:\n",
    "            # If the input text is a single list of words, convert it to\n",
    "            # list of lists for later iteration\n",
    "            if not _is_iterable_but_not_string(text[0]):\n",
    "                text = [text]\n",
    "        if labels is not None:\n",
    "            if not _is_iterable_but_not_string(labels):\n",
    "                raise ValueError(\n",
    "                    \"labels must be an iterable and not a string.\"\n",
    "                )\n",
    "            else:\n",
    "                if not _is_iterable_but_not_string(labels[0]):\n",
    "                    labels = [labels]\n",
    "\n",
    "        label_available = True\n",
    "        if labels is None:\n",
    "            label_available = False\n",
    "            # create an artificial label list for creating trailing token mask\n",
    "            labels = [[\"O\"] * len(t) for t in text]\n",
    "\n",
    "        input_ids_all = []\n",
    "        input_mask_all = []\n",
    "        label_ids_all = []\n",
    "        trailing_token_mask_all = []\n",
    "        for t, t_labels in zip(text, labels):\n",
    "\n",
    "            if len(t) != len(t_labels):\n",
    "                raise ValueError(\n",
    "                    \"The number of words is {0}, but the number of labels is {1}.\".format(\n",
    "                        len(t), len(t_labels)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            new_labels = []\n",
    "            new_tokens = []\n",
    "            if label_available:\n",
    "                for word, tag in zip(t, t_labels):\n",
    "                    sub_words = self.tokenizer.tokenize(word)\n",
    "                    for count, sub_word in enumerate(sub_words):\n",
    "                        if count > 0:\n",
    "                            tag = trailing_piece_tag\n",
    "                        new_labels.append(tag)\n",
    "                        new_tokens.append(sub_word)\n",
    "            else:\n",
    "                for word in t:\n",
    "                    sub_words = self.tokenizer.tokenize(word)\n",
    "                    for count, sub_word in enumerate(sub_words):\n",
    "                        if count > 0:\n",
    "                            tag = trailing_piece_tag\n",
    "                        else:\n",
    "                            tag = \"O\"\n",
    "                        new_labels.append(tag)\n",
    "                        new_tokens.append(sub_word)\n",
    "\n",
    "            if len(new_tokens) > max_len:\n",
    "                new_tokens = new_tokens[:max_len]\n",
    "                new_labels = new_labels[:max_len]\n",
    "            input_ids = self.tokenizer.convert_tokens_to_ids(new_tokens)\n",
    "\n",
    "            # The mask has 1 for real tokens and 0 for padding tokens.\n",
    "            # Only real tokens are attended to.\n",
    "            input_mask = [1.0] * len(input_ids)\n",
    "\n",
    "            # Zero-pad up to the max sequence length.\n",
    "            padding = [0.0] * (max_len - len(input_ids))\n",
    "            label_padding = [\"O\"] * (max_len - len(input_ids))\n",
    "\n",
    "            input_ids += padding\n",
    "            input_mask += padding\n",
    "            new_labels += label_padding\n",
    "\n",
    "            trailing_token_mask_all.append(\n",
    "                [\n",
    "                    True if label != trailing_piece_tag else False\n",
    "                    for label in new_labels\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            if label_map:\n",
    "                label_ids = [label_map[label] for label in new_labels]\n",
    "            else:\n",
    "                label_ids = new_labels\n",
    "\n",
    "            input_ids_all.append(input_ids)\n",
    "            input_mask_all.append(input_mask)\n",
    "            label_ids_all.append(label_ids)\n",
    "\n",
    "        if label_available:\n",
    "            return (\n",
    "                input_ids_all,\n",
    "                input_mask_all,\n",
    "                trailing_token_mask_all,\n",
    "                label_ids_all,\n",
    "            )\n",
    "        else:\n",
    "            return input_ids_all, input_mask_all, trailing_token_mask_all, None\n",
    "\n",
    "\n",
    "def create_data_loader(\n",
    "    input_ids,\n",
    "    input_mask,\n",
    "    label_ids=None,\n",
    "    sample_method=\"random\",\n",
    "    batch_size=32,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a dataloader for sampling and serving data batches.\n",
    "\n",
    "    Args:\n",
    "        input_ids (list): List of lists. Each sublist contains numerical\n",
    "            values, i.e. token ids, corresponding to the tokens in the input\n",
    "            text data.\n",
    "        input_mask (list): List of lists. Each sublist contains the attention\n",
    "            mask of the input token id list, 1 for input tokens and 0 for\n",
    "            padded tokens, so that padded tokens are not attended to.\n",
    "        label_ids (list, optional): List of lists of numerical labels,\n",
    "            each sublist contains token labels of a input\n",
    "            sentence/paragraph. Default value is None.\n",
    "        sample_method (str, optional): Order of data sampling. Accepted\n",
    "            values are \"random\", \"sequential\". Default value is \"random\".\n",
    "        batch_size (int, optional): Number of samples used in each training\n",
    "            iteration. Default value is 32.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: A Pytorch Dataloader containing the input_ids tensor,\n",
    "            input_mask tensor, and label_ids (if provided) tensor.\n",
    "\n",
    "    \"\"\"\n",
    "    input_ids_tensor = torch.tensor(input_ids, dtype=torch.long)\n",
    "    input_mask_tensor = torch.tensor(input_mask, dtype=torch.long)\n",
    "\n",
    "    if label_ids:\n",
    "        label_ids_tensor = torch.tensor(label_ids, dtype=torch.long)\n",
    "        tensor_data = TensorDataset(\n",
    "            input_ids_tensor, input_mask_tensor, label_ids_tensor\n",
    "        )\n",
    "    else:\n",
    "        tensor_data = TensorDataset(input_ids_tensor, input_mask_tensor)\n",
    "\n",
    "    if sample_method == \"random\":\n",
    "        sampler = RandomSampler(tensor_data)\n",
    "    elif sample_method == \"sequential\":\n",
    "        sampler = SequentialSampler(tensor_data)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Invalid sample_method value, accepted values are: \"\n",
    "            \"random, sequential, and distributed\"\n",
    "        )\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        tensor_data, sampler=sampler, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XLNET\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils_nlp.dataset.multinli import load_pandas_df\n",
    "from utils_nlp.eval.classification import eval_classification\n",
    "from utils_nlp.models.bert.sequence_classification import BERTSequenceClassifier\n",
    "from utils_nlp.common.timer import Timer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from pytorch_transformers import (WEIGHTS_NAME, XLNetConfig,XLNetForSequenceClassification,XLNetTokenizer)\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_FOLDER = \"../../../temp\"\n",
    "BERT_CACHE_DIR = \"../../../temp\"\n",
    "LANGUAGE = Language.ENGLISH\n",
    "TO_LOWER = True\n",
    "MAX_LEN = 150\n",
    "BATCH_SIZE = 32\n",
    "NUM_GPUS = 2\n",
    "NUM_EPOCHS = 1\n",
    "TRAIN_SIZE = 0.6\n",
    "LABEL_COL = \"genre\"\n",
    "TEXT_COL = \"sentence1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_pandas_df(DATA_FOLDER, \"train\")\n",
    "df = df[df[\"gold_label\"]==\"neutral\"]  # get unique sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/nlp_gpu/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# split\n",
    "df_train, df_test = train_test_split(df, train_size = TRAIN_SIZE, random_state=0)\n",
    "\n",
    "# encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_train = label_encoder.fit_transform(df_train[LABEL_COL])\n",
    "labels_test = label_encoder.transform(df_test[LABEL_COL])\n",
    "\n",
    "num_labels = len(np.unique(labels_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLTokenizer()\n",
    "tokens_train = tokenizer.tokenize(list(df_train[TEXT_COL]))\n",
    "tokens_test = tokenizer.tokenize(list(df_test[TEXT_COL]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_gpu_DSVM)",
   "language": "python",
   "name": "nlp_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
