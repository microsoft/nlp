{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) Microsoft Corporation. All rights reserved.*\n",
    "\n",
    "*Licensed under the MIT License.*\n",
    "\n",
    "# Sentiment analysis using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is obtained from the Text classification notebook\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils_nlp.dataset.multinli import load_pandas_df\n",
    "from utils_nlp.eval.classification import eval_classification\n",
    "from utils_nlp.models.bert.sequence_classification import BERTSequenceClassifier\n",
    "from utils_nlp.models.bert.common import Language, Tokenizer\n",
    "from utils_nlp.common.timer import Timer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we follow along with the [text-classification notebook](tc_mnli_bert.ipynb) to fine-tune BERT to perform sentiment analysis on the IMDB dataset [IMDB Large movie reviews](http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz) hosted by Stanford. The following data pre-processing is obtained from Google Research's example notebook for fine-tuning BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"../../../temp\"\n",
    "BERT_CACHE_DIR = \"../../../temp\"\n",
    "LANGUAGE = Language.ENGLISH\n",
    "TO_LOWER = True\n",
    "MAX_LEN = 150\n",
    "BATCH_SIZE = 32\n",
    "NUM_GPUS = 2\n",
    "NUM_EPOCHS = 1\n",
    "TRAIN_SIZE = 0.6\n",
    "LABEL_COL = \"polarity\"\n",
    "TEXT_COL = \"sentence\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_nlp.dataset.url_utils import maybe_download\n",
    "import tarfile\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from a directory\n",
    "\n",
    "# Download the dataset and load into pandas dataframe\n",
    "def download_or_find(url, directory=\".\", filename=\"aclImdb.tar.gz\"):\n",
    "    \"\"\"\n",
    "    Maybe download the data and put it into the given directory with given filename.\n",
    "    Skip the downloading if file already existed.\n",
    "    \n",
    "    Load the data into pandas Dataframe\n",
    "    Args:\n",
    "        url (string): The URL of the dataset\n",
    "        directory (string): Where to look for or store the dataset, default to current directory\n",
    "        filename (string): What filename to use for retrieve or store the dataset\n",
    "    \n",
    "    Return:\n",
    "        file_path (string): The file_path of the downloaded (or currently exists)\n",
    "    \"\"\"\n",
    "    print(\"=====> Begin downloading\")\n",
    "    file_path = maybe_download(url, filename, directory)\n",
    "    print(\"=====> Done downloading\")\n",
    "    \n",
    "    data_path = os.path.join(os.getcwd(), directory, \"aclImdb\")\n",
    "    \n",
    "    # Extract the data to the data folder\n",
    "    if not os.path.exists(data_path):\n",
    "        tar = tarfile.open(file_path)\n",
    "        tar.extractall(directory)\n",
    "        tar.close()\n",
    "    \n",
    "    # Return the path of dataset when done \n",
    "    print(\"=====> Finish extracting\")\n",
    "    return data_path\n",
    "    \n",
    "\n",
    "# Load all files from a directory in a DataFrame.\n",
    "def load_directory_data(directory):\n",
    "    \"\"\"\n",
    "    Method to go through all the files in the directory, get its content and put it into the appropriate train/test\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a new dictionary to store initial value for dataframe\n",
    "    data = {}\n",
    "    data[\"sentence\"] = []\n",
    "    data[\"sentiment\"] = []\n",
    "    \n",
    "    # Loop through all the subdirectories\n",
    "    for file_path in tqdm(os.listdir(directory)):\n",
    "        # Open each file\n",
    "        with open(os.path.join(directory, file_path), \"r\", encoding=\"utf8\") as f:\n",
    "            # Each file in the directory will be a text file (.txt) containing a review\n",
    "            data[\"sentence\"].append(f.read())\n",
    "            # The name of the file has 2 parts, the index of the file and the sentiment value of that review\n",
    "            # We only interested in the sentiment value, so only group(1) in the Match object\n",
    "            data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "    \n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "    print(\"===> Directory: {}\".format(directory))\n",
    "    # Load the positive and negative data to pandas Dataframe\n",
    "    pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "    neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "    \n",
    "    # Denoted positive to be 1 and negative be 0 for classification label\n",
    "    pos_df[\"polarity\"] = 1\n",
    "    neg_df[\"polarity\"] = 0\n",
    "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Download and process the dataset files.\n",
    "def download_and_load_datasets(force_download=False):\n",
    "    \n",
    "    URL = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "    \n",
    "    dataset_path = download_or_find(URL, directory=\"data\")\n",
    "\n",
    "    print(\"=============> Complete downloading\")\n",
    "    print(\"**** Dataset path: {}\".format(dataset_path))\n",
    "  \n",
    "    train_df = load_dataset(os.path.join(dataset_path, \"train\"))\n",
    "    \n",
    "    print(\"===> Complete train df\")\n",
    "\n",
    "    test_df = load_dataset(os.path.join(dataset_path, \"test\"))\n",
    "    print(\"===> Complete test df\")\n",
    "  \n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> Begin downloading\n",
      "=====> Done downloading\n",
      "=====> Finish extracting\n",
      "=============> Complete downloading\n",
      "**** Dataset path: C:\\Users\\ducl\\Documents\\GitHub\\nlp-2\\scenarios\\text_classification\\data\\aclImdb\n",
      "===> Directory: C:\\Users\\ducl\\Documents\\GitHub\\nlp-2\\scenarios\\text_classification\\data\\aclImdb\\train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96975c9bdd8a4243b935a8f75c64d081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8941cf4c2d471a86367b8948aa21eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> Complete train df\n",
      "===> Directory: C:\\Users\\ducl\\Documents\\GitHub\\nlp-2\\scenarios\\text_classification\\data\\aclImdb\\test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ed5f40fa93490d813645f955ad476b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d67875bc7114eab8f0369e83c61dffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> Complete test df\n"
     ]
    }
   ],
   "source": [
    "# Get the dataset and save it into pandas Dataframe\n",
    "train, test = download_and_load_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that this dataset contains movie reviews (as a whole) instead of individual sentences. Therefore, in the case of BERT, we should note that a single input in this case is a paragraph with potentially multiple sentences, compare to the original version where each input are a single sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is divided equally into 2 group of polarity, with 1 is positive review and 0 is negative review. The sentiment value are more detailed about the actual reaction rate, but we will experiemtn with it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.polarity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a sample of the data\n",
    "df_train = train.sample(5000)\n",
    "df_test = test.sample(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the class labels to make sure that we know which is which"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_train = label_encoder.fit_transform(df_train[LABEL_COL])\n",
    "labels_test = label_encoder.transform(df_test[LABEL_COL])\n",
    "\n",
    "num_labels = len(np.unique(labels_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique labels: {}\".format(num_labels))\n",
    "print(\"Number of training examples: {}\".format(df_train.shape[0]))\n",
    "print(\"Number of testing examples: {}\".format(df_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training, we need to transform the text data into a format that BERT understands. This process involves two steps. First, we instantiate a BERT tokenizer with a given language and then tokenize the text of the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(LANGUAGE, to_lower=TO_LOWER, cache_dir=BERT_CACHE_DIR)\n",
    "\n",
    "tokens_train = tokenizer.tokenize(list(df_train[TEXT_COL]))\n",
    "tokens_test = tokenizer.tokenize(list(df_test[TEXT_COL]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we perform the following preprocessing steps in the cell below:\n",
    "- Convert the tokens into token indices corresponding to the BERT tokenizer's vocabulary\n",
    "- Add the special tokens [CLS] and [SEP] to mark the beginning and end of a sentence\n",
    "- Pad or truncate the token lists to the specified max length\n",
    "- Return mask lists that indicate paddings' positions\n",
    "- Return token type id lists that indicate which sentence the tokens belong to (not needed for one-sequence classification)\n",
    "\n",
    "*See the original [implementation](https://github.com/google-research/bert/blob/master/run_classifier.py) for more information on BERT's input format.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train, mask_train, _ = tokenizer.preprocess_classification_tokens(\n",
    "    tokens_train, MAX_LEN\n",
    ")\n",
    "tokens_test, mask_test, _ = tokenizer.preprocess_classification_tokens(\n",
    "    tokens_test, MAX_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "Next, we create a sequence classifier that loads a pre-trained BERT model, given the language and number of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = BERTSequenceClassifier(\n",
    "    language=LANGUAGE, num_labels=num_labels, cache_dir=BERT_CACHE_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "We train the classifier using the training examples. This involves fine-tuning Hugging Face's PyTorch implementation of a pre-trained BERT transformer with a linear classifier layer attached to perform sequence classification tasks. The arguments for fitting the classifier are:\n",
    "- token_ids: list of token indices\n",
    "- input_mask: mask lists that indicate paddings' position\n",
    "- labels: list of training labels\n",
    "- num_gpus: number of GPUs. If none specified, all available GPUs will be used\n",
    "- num_epochs: number of training epochs (default 1)\n",
    "- batch_size: training batch size (default 32)\n",
    "- verbose: displays training progress and loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/1; batch:1->16/156; loss:0.696118\n",
      "epoch:1/1; batch:17->32/156; loss:0.362434\n",
      "epoch:1/1; batch:33->48/156; loss:0.358708\n",
      "epoch:1/1; batch:49->64/156; loss:0.350601\n",
      "epoch:1/1; batch:65->80/156; loss:0.397891\n",
      "epoch:1/1; batch:81->96/156; loss:0.483194\n",
      "epoch:1/1; batch:97->112/156; loss:0.236887\n",
      "epoch:1/1; batch:113->128/156; loss:0.300213\n",
      "epoch:1/1; batch:129->144/156; loss:0.511190\n",
      "epoch:1/1; batch:145->156/156; loss:0.113811\n",
      "[Training time: 1.165 hrs]\n"
     ]
    }
   ],
   "source": [
    "with Timer() as t:\n",
    "    classifier.fit(\n",
    "        token_ids=tokens_train,\n",
    "        input_mask=mask_train,\n",
    "        labels=labels_train,    \n",
    "        num_gpus=NUM_GPUS,        \n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        batch_size=BATCH_SIZE,    \n",
    "        verbose=True,\n",
    "    )    \n",
    "print(\"[Training time: {:.3f} hrs]\".format(t.interval / 3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score\n",
    "We score the test set using the trained classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                         | 0/5000 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▌                                                                               | 32/5000 [00:09<23:24,  3.54it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|█                                                                               | 64/5000 [00:17<23:05,  3.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|█▌                                                                              | 96/5000 [00:26<22:52,  3.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|██                                                                             | 128/5000 [00:35<22:47,  3.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|██▌                                                                            | 160/5000 [00:44<22:45,  3.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|███                                                                            | 192/5000 [00:53<22:30,  3.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|███▌                                                                           | 224/5000 [01:02<22:25,  3.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|████                                                                           | 256/5000 [01:11<22:16,  3.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|████▌                                                                          | 288/5000 [01:20<21:50,  3.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|█████                                                                          | 320/5000 [01:29<21:34,  3.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|█████▌                                                                         | 352/5000 [01:38<21:22,  3.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|██████                                                                         | 384/5000 [01:46<21:10,  3.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|██████▌                                                                        | 416/5000 [01:55<20:54,  3.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|███████                                                                        | 448/5000 [02:05<21:19,  3.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|███████▌                                                                       | 480/5000 [02:14<21:23,  3.52it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|████████                                                                       | 512/5000 [02:23<21:19,  3.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|████████▌                                                                      | 544/5000 [02:32<21:17,  3.49it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|█████████                                                                      | 576/5000 [02:41<21:01,  3.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|█████████▌                                                                     | 608/5000 [02:50<20:36,  3.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|██████████                                                                     | 640/5000 [02:59<20:19,  3.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|██████████▌                                                                    | 672/5000 [03:08<20:13,  3.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|███████████                                                                    | 704/5000 [03:17<19:58,  3.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|███████████▋                                                                   | 736/5000 [03:26<19:45,  3.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|████████████▏                                                                  | 768/5000 [03:34<19:34,  3.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|████████████▋                                                                  | 800/5000 [03:43<19:30,  3.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|█████████████▏                                                                 | 832/5000 [03:52<19:20,  3.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|█████████████▋                                                                 | 864/5000 [04:01<19:16,  3.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|██████████████▏                                                                | 896/5000 [04:10<19:04,  3.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|██████████████▋                                                                | 928/5000 [04:19<18:55,  3.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████████████▏                                                               | 960/5000 [04:28<18:44,  3.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████████████▋                                                               | 992/5000 [04:37<18:34,  3.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████████████▉                                                              | 1024/5000 [04:46<18:25,  3.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████████████▍                                                             | 1056/5000 [04:55<18:24,  3.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████████████▉                                                             | 1088/5000 [05:04<18:17,  3.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|█████████████████▍                                                            | 1120/5000 [05:13<18:10,  3.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|█████████████████▉                                                            | 1152/5000 [05:22<18:03,  3.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|██████████████████▍                                                           | 1184/5000 [05:31<17:43,  3.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|██████████████████▉                                                           | 1216/5000 [05:40<17:31,  3.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|███████████████████▍                                                          | 1248/5000 [05:49<17:28,  3.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|███████████████████▉                                                          | 1280/5000 [05:58<17:22,  3.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|████████████████████▍                                                         | 1312/5000 [06:07<17:15,  3.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|████████████████████▉                                                         | 1344/5000 [06:16<17:10,  3.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|█████████████████████▍                                                        | 1376/5000 [06:25<17:18,  3.49it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|█████████████████████▉                                                        | 1408/5000 [06:34<17:03,  3.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|██████████████████████▍                                                       | 1440/5000 [06:43<16:45,  3.54it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|██████████████████████▉                                                       | 1472/5000 [06:52<16:26,  3.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███████████████████████▍                                                      | 1504/5000 [07:01<16:12,  3.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███████████████████████▉                                                      | 1536/5000 [07:09<16:01,  3.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|████████████████████████▍                                                     | 1568/5000 [07:19<15:59,  3.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|████████████████████████▉                                                     | 1600/5000 [07:28<15:55,  3.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|█████████████████████████▍                                                    | 1632/5000 [07:37<15:46,  3.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|█████████████████████████▉                                                    | 1664/5000 [07:46<15:35,  3.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|██████████████████████████▍                                                   | 1696/5000 [07:55<15:27,  3.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|██████████████████████████▉                                                   | 1728/5000 [08:03<15:08,  3.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|███████████████████████████▍                                                  | 1760/5000 [08:12<15:00,  3.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|███████████████████████████▉                                                  | 1792/5000 [08:21<14:51,  3.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|████████████████████████████▍                                                 | 1824/5000 [08:31<15:06,  3.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|████████████████████████████▉                                                 | 1856/5000 [08:40<14:57,  3.50it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|█████████████████████████████▍                                                | 1888/5000 [08:49<14:44,  3.52it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|█████████████████████████████▉                                                | 1920/5000 [08:58<14:28,  3.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|██████████████████████████████▍                                               | 1952/5000 [09:06<14:09,  3.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|██████████████████████████████▉                                               | 1984/5000 [09:16<14:06,  3.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████████████████████████▍                                              | 2016/5000 [09:25<13:58,  3.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████████████████████▉                                              | 2048/5000 [09:34<13:51,  3.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|████████████████████████████████▍                                             | 2080/5000 [09:43<13:52,  3.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|████████████████████████████████▉                                             | 2112/5000 [09:52<13:47,  3.49it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|█████████████████████████████████▍                                            | 2144/5000 [10:01<13:36,  3.50it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|█████████████████████████████████▉                                            | 2176/5000 [10:10<13:22,  3.52it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|██████████████████████████████████▍                                           | 2208/5000 [10:20<13:23,  3.48it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|██████████████████████████████████▉                                           | 2240/5000 [10:29<13:06,  3.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|███████████████████████████████████▍                                          | 2272/5000 [10:38<12:57,  3.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|███████████████████████████████████▉                                          | 2304/5000 [10:47<12:46,  3.52it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|████████████████████████████████████▍                                         | 2336/5000 [10:56<12:40,  3.50it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|████████████████████████████████████▉                                         | 2368/5000 [11:05<12:34,  3.49it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|█████████████████████████████████████▍                                        | 2400/5000 [11:14<12:23,  3.50it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|█████████████████████████████████████▉                                        | 2432/5000 [11:23<12:10,  3.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|██████████████████████████████████████▍                                       | 2464/5000 [11:32<11:58,  3.53it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|██████████████████████████████████████▉                                       | 2496/5000 [11:41<11:45,  3.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████████████████████████▍                                      | 2528/5000 [11:50<11:37,  3.54it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████████████████████████▉                                      | 2560/5000 [12:00<11:31,  3.53it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|████████████████████████████████████████▍                                     | 2592/5000 [12:09<11:19,  3.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|████████████████████████████████████████▉                                     | 2624/5000 [12:18<11:14,  3.52it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|█████████████████████████████████████████▍                                    | 2656/5000 [12:27<11:06,  3.52it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████████████████████████████████████████▉                                    | 2688/5000 [12:36<10:53,  3.54it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|██████████████████████████████████████████▍                                   | 2720/5000 [12:45<10:42,  3.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|██████████████████████████████████████████▉                                   | 2752/5000 [12:53<10:26,  3.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|███████████████████████████████████████████▍                                  | 2784/5000 [13:02<10:15,  3.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|███████████████████████████████████████████▉                                  | 2816/5000 [13:12<10:19,  3.53it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|████████████████████████████████████████████▍                                 | 2848/5000 [13:21<10:12,  3.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|████████████████████████████████████████████▉                                 | 2880/5000 [13:30<10:04,  3.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|█████████████████████████████████████████████▍                                | 2912/5000 [13:39<09:55,  3.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|█████████████████████████████████████████████▉                                | 2944/5000 [13:48<09:38,  3.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████████████████████████████████████████████▍                               | 2976/5000 [13:57<09:28,  3.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████████████████████████████████████████████▉                               | 3008/5000 [14:06<09:16,  3.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████████████████████████████▍                              | 3040/5000 [14:14<09:05,  3.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████████████████████████████▉                              | 3072/5000 [14:23<08:56,  3.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|████████████████████████████████████████████████▍                             | 3104/5000 [14:33<08:51,  3.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|████████████████████████████████████████████████▉                             | 3136/5000 [14:42<08:43,  3.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|█████████████████████████████████████████████████▍                            | 3168/5000 [14:51<08:34,  3.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|█████████████████████████████████████████████████▉                            | 3200/5000 [14:59<08:24,  3.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|██████████████████████████████████████████████████▍                           | 3232/5000 [15:08<08:13,  3.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|██████████████████████████████████████████████████▉                           | 3264/5000 [15:17<08:02,  3.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|███████████████████████████████████████████████████▍                          | 3296/5000 [15:26<07:53,  3.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|███████████████████████████████████████████████████▉                          | 3328/5000 [15:35<07:46,  3.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|████████████████████████████████████████████████████▍                         | 3360/5000 [15:44<07:38,  3.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|████████████████████████████████████████████████████▉                         | 3392/5000 [15:54<07:39,  3.50it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|█████████████████████████████████████████████████████▍                        | 3424/5000 [16:03<07:30,  3.50it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|█████████████████████████████████████████████████████▉                        | 3456/5000 [16:12<07:21,  3.50it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████████████████████████████████████████████████████▍                       | 3488/5000 [16:21<07:07,  3.53it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████████████████████████████████████████████████████▉                       | 3520/5000 [16:30<06:56,  3.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████████████████████████████████▍                      | 3552/5000 [16:38<06:43,  3.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████████████████████████████████████████████████████▉                      | 3584/5000 [16:47<06:35,  3.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|████████████████████████████████████████████████████████▍                     | 3616/5000 [16:56<06:25,  3.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████████████████████████████████████████▉                     | 3648/5000 [17:05<06:17,  3.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|█████████████████████████████████████████████████████████▍                    | 3680/5000 [17:14<06:08,  3.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|█████████████████████████████████████████████████████████▉                    | 3712/5000 [17:23<05:59,  3.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|██████████████████████████████████████████████████████████▍                   | 3744/5000 [17:32<05:54,  3.54it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|██████████████████████████████████████████████████████████▉                   | 3776/5000 [17:41<05:46,  3.53it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|███████████████████████████████████████████████████████████▍                  | 3808/5000 [17:51<05:37,  3.53it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|███████████████████████████████████████████████████████████▉                  | 3840/5000 [18:00<05:28,  3.53it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|████████████████████████████████████████████████████████████▍                 | 3872/5000 [18:09<05:18,  3.54it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|████████████████████████████████████████████████████████████▉                 | 3904/5000 [18:18<05:09,  3.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████████████████████████████████████▍                | 3936/5000 [18:26<04:59,  3.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████████████████████████████████████▉                | 3968/5000 [18:35<04:48,  3.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████████████████████████████████████▍               | 4000/5000 [18:44<04:38,  3.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████████████████████████████████████▉               | 4032/5000 [18:53<04:30,  3.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|███████████████████████████████████████████████████████████████▍              | 4064/5000 [19:02<04:23,  3.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████████████████████████████████████▉              | 4096/5000 [19:12<04:16,  3.52it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|████████████████████████████████████████████████████████████████▍             | 4128/5000 [19:21<04:07,  3.52it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|████████████████████████████████████████████████████████████████▉             | 4160/5000 [19:30<03:58,  3.52it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|█████████████████████████████████████████████████████████████████▍            | 4192/5000 [19:39<03:47,  3.54it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|█████████████████████████████████████████████████████████████████▉            | 4224/5000 [19:48<03:38,  3.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|██████████████████████████████████████████████████████████████████▍           | 4256/5000 [19:57<03:30,  3.53it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|██████████████████████████████████████████████████████████████████▉           | 4288/5000 [20:06<03:22,  3.52it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|███████████████████████████████████████████████████████████████████▍          | 4320/5000 [20:15<03:13,  3.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|███████████████████████████████████████████████████████████████████▉          | 4352/5000 [20:24<03:05,  3.49it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████████████████████████████████████████████████████████████████▍         | 4384/5000 [20:33<02:55,  3.50it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████████████████████████████████████████████████████████████████▉         | 4416/5000 [20:42<02:44,  3.54it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████████████████████████████████████████▍        | 4448/5000 [20:51<02:35,  3.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|█████████████████████████████████████████████████████████████████████▉        | 4480/5000 [21:00<02:26,  3.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████████████████████████████████████████▍       | 4512/5000 [21:09<02:16,  3.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████████████████████████████████████████▉       | 4544/5000 [21:18<02:06,  3.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████████████████████████████████████████▍      | 4576/5000 [21:26<01:57,  3.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████████████████████████████████████████▉      | 4608/5000 [21:35<01:48,  3.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|████████████████████████████████████████████████████████████████████████▍     | 4640/5000 [21:44<01:39,  3.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|████████████████████████████████████████████████████████████████████████▉     | 4672/5000 [21:53<01:31,  3.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|█████████████████████████████████████████████████████████████████████████▍    | 4704/5000 [22:02<01:22,  3.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|█████████████████████████████████████████████████████████████████████████▉    | 4736/5000 [22:11<01:13,  3.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|██████████████████████████████████████████████████████████████████████████▍   | 4768/5000 [22:20<01:05,  3.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|██████████████████████████████████████████████████████████████████████████▉   | 4800/5000 [22:29<00:56,  3.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|███████████████████████████████████████████████████████████████████████████▍  | 4832/5000 [22:38<00:47,  3.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|███████████████████████████████████████████████████████████████████████████▉  | 4864/5000 [22:47<00:38,  3.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|████████████████████████████████████████████████████████████████████████████▍ | 4896/5000 [22:56<00:28,  3.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|████████████████████████████████████████████████████████████████████████████▉ | 4928/5000 [23:05<00:20,  3.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████████████████████████████████████████████████████████████████████████▍| 4960/5000 [23:14<00:11,  3.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████▉| 4992/5000 [23:23<00:02,  3.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5024it [23:25,  4.55it/s]                                                                                              "
     ]
    }
   ],
   "source": [
    "preds = classifier.predict(\n",
    "    token_ids=tokens_test, input_mask=mask_test, num_gpus=NUM_GPUS, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Results\n",
    "Finally, we compute the accuracy, precision, recall, and F1 metrics of the evaluation on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels_test, preds, target_names=[\"negative\", \"positive\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
