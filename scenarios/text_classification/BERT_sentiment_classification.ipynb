{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License.\n",
    "\n",
    "# Sentiment analysis using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we fine-tune a pretrained [BERT](https://arxiv.org/abs/1810.04805) model to perform sentiment analysis on the IMDb dataset [IMDb Large movie reviews](http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz). You'll find the following contents:\n",
    "* Data preprocessing and tokenization\n",
    "* Creating and training a model\n",
    "* Scoring and evaluating results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT basics\n",
    "\n",
    "BERT is a language representation model that effectively captures deep and subtle textual relationships in a corpus. In the [original paper](https://arxiv.org/abs/1810.04805), the authors demonstrate that the BERT model could be easily adapted to build state-of-the-art models for a number of NLP tasks, including text classification, named entity recognition and question answering. \n",
    "\n",
    "We fine-tune BERT for sentiment classification by wrapping a pre-trained BERT model with a [sequence classifier](../../utils_nlp/bert/sequence_classification.py). Although we use [Hugging Face's PyTorch implementation](https://github.com/huggingface/pytorch-pretrained-BERT) of [BERT](https://github.com/google-research/bert) in this notebook, the [AzureML introduction to BERT](https://github.com/microsoft/AzureML-BERT/blob/master/docs/bert-intro.md) provides a useful overview of BERT concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDb dataset\n",
    "\n",
    "The [IMDb Large movie reviews](http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz) dataset for binary sentiment classification contains positive and negative movie reviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the [setup instructions](http://localhost:8888/edit/SETUP.md) in this repo. Then run the following cell to make sure you installed all the packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils_nlp.dataset.bert_sentiment import download_and_load_datasets\n",
    "from utils_nlp.eval.classification import eval_classification\n",
    "from utils_nlp.models.bert.sequence_classification import BERTSequenceClassifier\n",
    "from utils_nlp.models.bert.common import Language, Tokenizer\n",
    "from utils_nlp.common.timer import Timer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set global constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_CACHE_DIR = \"../../../temp\"\n",
    "LANGUAGE = Language.ENGLISH \n",
    "TO_LOWER = True # All text to lowercase\n",
    "MAX_LEN = 150 # Max length of a single movie reviews\n",
    "BATCH_SIZE = 32\n",
    "NUM_GPUS = 2 # Change to match your system\n",
    "NUM_EPOCHS = 2\n",
    "TRAIN_SIZE = 0.6\n",
    "LABEL_COL = \"polarity\" # Positive or negative movie review\n",
    "TEXT_COL = \"sentence\" # Text of the movie review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dataset and save it into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> Begin downloading\n",
      "=====> Done downloading\n",
      "=====> Finish extracting\n",
      "=============> Complete downloading\n",
      "**** Dataset path: C:\\Users\\ducl\\Documents\\GitHub\\nlp-2\\scenarios\\text_classification\\data\\aclImdb\n",
      "===> Directory: C:\\Users\\ducl\\Documents\\GitHub\\nlp-2\\scenarios\\text_classification\\data\\aclImdb\\train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c27cbf38a7b24319a414725dfe3ad24b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e36e7bd7c2a4857808514c82ac9f318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> Complete train df\n",
      "===> Directory: C:\\Users\\ducl\\Documents\\GitHub\\nlp-2\\scenarios\\text_classification\\data\\aclImdb\\test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e407ce9d1ee8478f9055117e2c865a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e712a9beff4891a95bf56d874e2a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> Complete test df\n"
     ]
    }
   ],
   "source": [
    "train, test = download_and_load_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IMDb dataset contains whole movie reviews. One input is one movie review in paragraph form with one or more sentences. The sentiment column shows how positive the movie review is on a scale from 1 to 10. The polarity column shows whether it is classified as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I loved this movie. It is a definite inspirati...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Obviously, I didn't care for Things to Come (a...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Protégé runs in a linear fashion; expect no fa...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Utter dreck. I got to the 16 minute/27 second ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, there you have it, another disillusion o...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence sentiment  polarity\n",
       "0  I loved this movie. It is a definite inspirati...        10         1\n",
       "1  Obviously, I didn't care for Things to Come (a...         4         0\n",
       "2  Protégé runs in a linear fashion; expect no fa...         3         0\n",
       "3  Utter dreck. I got to the 16 minute/27 second ...         1         0\n",
       "4  Well, there you have it, another disillusion o...         2         0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 3 columns):\n",
      "sentence     25000 non-null object\n",
      "sentiment    25000 non-null object\n",
      "polarity     25000 non-null int64\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 586.0+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is divided equally into 2 groups of polarity. 1 indicates positive reviews and 0 indicates negative reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12500\n",
       "0    12500\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.polarity.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently we train and test on the full datasets. To run a faster experiment, reduce the sample fraction from 1.0 to 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train.sample(frac=1.0)\n",
    "df_test = test.sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the positive and negative class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "labels_train = label_encoder.fit_transform(df_train[LABEL_COL])\n",
    "labels_test = label_encoder.transform(df_test[LABEL_COL])\n",
    "\n",
    "num_labels = len(np.unique(labels_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n",
      "Number of training examples: 5000\n",
      "Number of testing examples: 5000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique labels: {}\".format(num_labels))\n",
    "print(\"Number of training examples: {}\".format(df_train.shape[0]))\n",
    "print(\"Number of testing examples: {}\".format(df_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training, transform the text data into a format that BERT understands. \n",
    "\n",
    "First, instantiate a BERT tokenizer with a given language. We'll use this to tokenize the text of the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:15<00:00, 322.83it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:15<00:00, 315.00it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(LANGUAGE, to_lower=TO_LOWER, cache_dir=BERT_CACHE_DIR)\n",
    "\n",
    "tokens_train = tokenizer.tokenize(list(df_train[TEXT_COL]))\n",
    "tokens_test = tokenizer.tokenize(list(df_test[TEXT_COL]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, perform the following preprocessing steps:\n",
    "* Convert the tokens into token indices corresponding to the BERT tokenizer's vocabulary\n",
    "* Add the special tokens [CLS] and [SEP] to mark the beginning and end of a sentence\n",
    "* Pad or truncate the token lists to the specified max length\n",
    "* Return mask lists that indicate paddings' positions\n",
    "* Return token type id lists that indicate which sentence the tokens belong to (not needed for one-sequence classification)\n",
    "\n",
    "*See the original [implementation](https://github.com/google-research/bert/blob/master/run_classifier.py) for more information on BERT's input format.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train, mask_train, _ = tokenizer.preprocess_classification_tokens(\n",
    "    tokens_train, MAX_LEN\n",
    ")\n",
    "tokens_test, mask_test, _ = tokenizer.preprocess_classification_tokens(\n",
    "    tokens_test, MAX_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model\n",
    "Create a sequence classifier that loads a pre-trained BERT model given the language and number of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = BERTSequenceClassifier(\n",
    "    language=LANGUAGE, num_labels=num_labels, cache_dir=BERT_CACHE_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the classifier\n",
    "Attach a linear classifier layer to the pre-trained BERT transformer to perform sequence classification tasks on the training data. The arguments for fitting the classifier are:\n",
    "* token_ids: list of token indices\n",
    "* input_mask: mask lists that indicate paddings' position\n",
    "* labels: list of training labels\n",
    "* num_gpus: number of GPUs. If none specified, all available GPUs will be used\n",
    "* num_epochs: number of training epochs (default 1)\n",
    "* batch_size: training batch size (default 32)\n",
    "* verbose: displays training progress and loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - t_total value of -1 results in schedule not being applied\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/2; batch:1->16/156; loss:0.688796\n",
      "epoch:1/2; batch:17->32/156; loss:0.456730\n",
      "epoch:1/2; batch:33->48/156; loss:0.304804\n",
      "epoch:1/2; batch:49->64/156; loss:0.320006\n",
      "epoch:1/2; batch:65->80/156; loss:0.207728\n",
      "epoch:1/2; batch:81->96/156; loss:0.013612\n"
     ]
    }
   ],
   "source": [
    "# with Timer() as t:\n",
    "    classifier.fit(\n",
    "        token_ids=tokens_train,\n",
    "        input_mask=mask_train,\n",
    "        labels=labels_train,    \n",
    "        num_gpus=0,        \n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        batch_size=BATCH_SIZE,    \n",
    "        verbose=True,\n",
    "    )    \n",
    "# print(\"[Training time: {:.3f} hrs]\".format(t.interval / 3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score\n",
    "Score the test set using the trained classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = classifier.predict(\n",
    "    token_ids=tokens_test, input_mask=mask_test, num_gpus=NUM_GPUS, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate results\n",
    "Finally, compute the accuracy, precision, recall, and F1 metrics of the evaluation on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels_test, preds, target_names=[\"negative\", \"positive\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
