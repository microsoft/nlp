{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "In this notebook we show how to apply common preprocessing steps (lowercase standardization and tokenization) to the  [SNLI](https://nlp.stanford.edu/projects/snli/) corpus. We also reshape the data into the form needed to implement [Gensen](https://github.com/Maluuba/gensen), a model that learns rich fixed-length sentence embeddings as described in the paper [here](https://openreview.net/forum?id=B18WgG-CZ&noteId=B18WgG-CZ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 00 Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../../\")\n",
    "\n",
    "import os\n",
    "from utils_nlp.dataset.preprocess import to_lowercase, to_nltk_tokens\n",
    "from utils_nlp.dataset import snli\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DATA_PATH  = '../../../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 Tokenize\n",
    "\n",
    "We first load the dataset and convert it to a pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract data\n",
    "train = snli.load_pandas_df(BASE_DATA_PATH, file_split='train')\n",
    "dev = snli.load_pandas_df(BASE_DATA_PATH, file_split='dev')\n",
    "test = snli.load_pandas_df(BASE_DATA_PATH, file_split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also clean the data before tokenizing. This includes dropping unneccessary columns and renaming the relevant columns as score, sentence_1, and sentence_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df, file_split):\n",
    "    src_file_path = os.path.join(BASE_DATA_PATH, \"raw/snli_1.0/snli_1.0_{}.txt\".format(file_split))\n",
    "    dest_file_path = os.path.join(BASE_DATA_PATH, \"clean/snli_1.0/snli_1.0_clean_{}.txt\".format(file_split))\n",
    "    return snli.clean_snli(src_file_path, dest_file_path).dropna() # drop rows with any NaN vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = clean(train, 'train')\n",
    "dev = clean(dev, 'dev')\n",
    "test = clean(test, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the clean pandas dataframes, we do lowercase standardization and tokenization. We use the [NLTK](https://www.nltk.org/) library for tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/caseyhong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/caseyhong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/caseyhong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "train_tok = to_nltk_tokens(to_lowercase(train))\n",
    "dev_tok = to_nltk_tokens(to_lowercase(dev))\n",
    "test_tok = to_nltk_tokens(to_lowercase(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>sentence1_tokens</th>\n",
       "      <th>sentence2_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>two women are embracing while holding to go pa...</td>\n",
       "      <td>the sisters are hugging goodbye while holding ...</td>\n",
       "      <td>[two, women, are, embracing, while, holding, t...</td>\n",
       "      <td>[the, sisters, are, hugging, goodbye, while, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>entailment</td>\n",
       "      <td>two women are embracing while holding to go pa...</td>\n",
       "      <td>two woman are holding packages.</td>\n",
       "      <td>[two, women, are, embracing, while, holding, t...</td>\n",
       "      <td>[two, woman, are, holding, packages, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>contradiction</td>\n",
       "      <td>two women are embracing while holding to go pa...</td>\n",
       "      <td>the men are fighting outside a deli.</td>\n",
       "      <td>[two, women, are, embracing, while, holding, t...</td>\n",
       "      <td>[the, men, are, fighting, outside, a, deli, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>entailment</td>\n",
       "      <td>two young children in blue jerseys, one with t...</td>\n",
       "      <td>two kids in numbered jerseys wash their hands.</td>\n",
       "      <td>[two, young, children, in, blue, jerseys, ,, o...</td>\n",
       "      <td>[two, kids, in, numbered, jerseys, wash, their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>two young children in blue jerseys, one with t...</td>\n",
       "      <td>two kids at a ballgame wash their hands.</td>\n",
       "      <td>[two, young, children, in, blue, jerseys, ,, o...</td>\n",
       "      <td>[two, kids, at, a, ballgame, wash, their, hand...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           score                                          sentence1  \\\n",
       "0        neutral  two women are embracing while holding to go pa...   \n",
       "1     entailment  two women are embracing while holding to go pa...   \n",
       "2  contradiction  two women are embracing while holding to go pa...   \n",
       "3     entailment  two young children in blue jerseys, one with t...   \n",
       "4        neutral  two young children in blue jerseys, one with t...   \n",
       "\n",
       "                                           sentence2  \\\n",
       "0  the sisters are hugging goodbye while holding ...   \n",
       "1                    two woman are holding packages.   \n",
       "2               the men are fighting outside a deli.   \n",
       "3     two kids in numbered jerseys wash their hands.   \n",
       "4           two kids at a ballgame wash their hands.   \n",
       "\n",
       "                                    sentence1_tokens  \\\n",
       "0  [two, women, are, embracing, while, holding, t...   \n",
       "1  [two, women, are, embracing, while, holding, t...   \n",
       "2  [two, women, are, embracing, while, holding, t...   \n",
       "3  [two, young, children, in, blue, jerseys, ,, o...   \n",
       "4  [two, young, children, in, blue, jerseys, ,, o...   \n",
       "\n",
       "                                    sentence2_tokens  \n",
       "0  [the, sisters, are, hugging, goodbye, while, h...  \n",
       "1            [two, woman, are, holding, packages, .]  \n",
       "2     [the, men, are, fighting, outside, a, deli, .]  \n",
       "3  [two, kids, in, numbered, jerseys, wash, their...  \n",
       "4  [two, kids, at, a, ballgame, wash, their, hand...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_tok.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 Reshape\n",
    "We need to prepare our data in a specific way in order for the Gensen model to be able to ingest it. We do this by\n",
    "* Saving the tokens for each split in a `snli_1.0_{split}.txt.clean` file, with the sentence pairs and scores tab-separated and the tokens separated by a single space.\n",
    "* Saving the tokenized sentence and labels separately, in the form `snli_1.0_{split}.txt.s1.tok` or `snli_1.0_{split}.txt.s2.tok` or `snli_1.0_{split}.txt.lab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_map = {'train': train_tok, 'dev': dev_tok, 'test': test_tok}\n",
    "for file_split, df in split_map.items():\n",
    "    base_txt_path = os.path.join(BASE_DATA_PATH, \"clean/snli_1.0/snli_1.0_{}.txt\".format(file_split))\n",
    "    df['s1.tok'] = df['sentence1_tokens'].apply(lambda x: ' '.join(x))\n",
    "    df['s2.tok'] = df['sentence2_tokens'].apply(lambda x: ' '.join(x))\n",
    "    df['s1.tok'].to_csv(\"{}.s1.tok\".format(base_txt_path), sep=' ', header=False, index=False)\n",
    "    df['s2.tok'].to_csv(\"{}.s2.tok\".format(base_txt_path), sep=' ', header=False, index=False)\n",
    "    df['score'].to_csv(\"{}.lab\".format(base_txt_path), sep=' ', header=False, index=False)\n",
    "    df[['s1.tok', 's2.tok', 'score']].to_csv(\"{}.clean\".format(base_txt_path), sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove quotations from .tok files\n",
    "import shutil\n",
    "\n",
    "for file_split in split_map.keys():\n",
    "    s1_tok_path = os.path.join(BASE_DATA_PATH, \"clean/snli_1.0/snli_1.0_{}.txt.s1.tok\".format(file_split))\n",
    "    s2_tok_path = os.path.join(BASE_DATA_PATH, \"clean/snli_1.0/snli_1.0_{}.txt.s2.tok\".format(file_split))\n",
    "    with open(s1_tok_path, 'r') as fin, open(\"{}.tmp\".format(s1_tok_path), 'w') as tmp:\n",
    "        for line in fin:\n",
    "            s = line.replace('\\\"', '')\n",
    "            tmp.write(s)\n",
    "    with open(s2_tok_path, 'r') as fin, open(\"{}.tmp\".format(s2_tok_path), 'w') as tmp:\n",
    "        for line in fin:\n",
    "            s = line.replace('\\\"', '')\n",
    "            tmp.write(s)\n",
    "    shutil.move(\"{}.tmp\".format(s1_tok_path), s1_tok_path)\n",
    "    shutil.move(\"{}.tmp\".format(s2_tok_path), s2_tok_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
