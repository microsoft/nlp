{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Baseline Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a baseline model? \n",
    "\n",
    "Producing a baseline model is crucial for evaluating your model's performance on any machine learning problem. A baseline model is a basic solution to a machine learning problem that serves as a point of reference for comparing other models to. The baseline model's performance gives us an indication of how much better our models can perform relative to a naive approach. \n",
    "\n",
    "Let's say we are building a sentence similarity model where our training set contains pairs of sentences and we want to predict how similiar these sentences are on a scale from 1-5. We could spend months producing a complex machine learning solution to this problem and ultimately get a mean squared error (MSE) of 0.3. But is this result good or bad? There is no way of knowing without comparing it with some baseline performance. For our baseline model, we could predict the mean sentence similarity of sentence pairs in our training set (called the _zero rule_) and get a MSE of 0.35. So our model is worse than the baseline which indicates that we may want to consider using different features, models, evaluation metrics, etc. It is crucial that the choice of baseline model be tailored to a data science problem based on buisness goals and the specific modeling task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are good baselines for sentence similarity?\n",
    "\n",
    "For sentence similarity problems, we have two sub-tasks: 1) First, we need to produce a vector representation of each sentence in the sentence pair, known as an **embedding**. 2) Second, we need to compute the similarity between these two sentence embeddings.\n",
    "\n",
    "For producing representations of sentences, there are some common baseline approaches: \n",
    "1. Create word embeddings for each word in a sentence\n",
    "    1. word2vec word embeddings\n",
    "    2. GLoVe word embeddings\n",
    "    3. fastText word embeddings\n",
    "    \n",
    "2. Create sentence embeddings\n",
    "    1. doc2vec document embeddings\n",
    "    2. TF-IDF embeddings \n",
    "\n",
    "Then we have to compare our embeddings to calculate sentence similarity:\n",
    "1. Word Embedding comparison\n",
    "    1. Cosine Similarity (first requires averaging the word embeddings of all words in each sentence)\n",
    "    2. Word Mover's Distance\n",
    "\n",
    "2. Sentence Embedding comparison\n",
    "    1. Cosine Similarity  \n",
    "    \n",
    "The different embedding models and similarity metrics are introduced below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Data Loading and Preprocessing](#Data-Loading-and-Preprocessing)\n",
    "    * [Load STS Benchmark Dataset](#Load-STS-Benchmark-Dataset)\n",
    "    - [Preprocess / Tokenize](#Data-Preprocessing-/-Tokenization)\n",
    "    - [Document Frequency Calculation](#Document-Frequency-Calculation)\n",
    "* [Baseline Models](#Baseline-Models)\n",
    "    - [Baseline #1: word2vec and cosine similarity](#Baseline-#1:-word2vec-embeddings-+-cosine-similarity-(word2vec-+-cosine))\n",
    "    - [Baseline #2: word2vec and Word Mover's Distance](#Baseline-#2:-word2vec-embeddings-+-Word-Mover's-Distance-(word2vec-+-WMD))\n",
    "    - [Baseline #3: GloVe and cosine similarity](#Baseline-#3:-GloVe-embeddings-+-cosine-similarity-(GloVe-+-cosine))\n",
    "    * [Baseline #4: GloVe and Word Mover's Distance](#Baseline-#4:-GloVe-embeddings-+-Word-Mover's-Distance-(GloVe-+-WMD))\n",
    "    - [Baseline #5: fastText and cosine similarity](#Baseline-#5:-fastText-embeddings-+-cosine-similarity-(fastText-+-cosine))\n",
    "    - [Baseline #6: fastText and Word Mover's Distance](#Baseline-#6:-fastText-embeddings-+-Word-Mover's-Distance-(fastText-+-WMD))\n",
    "\n",
    "    * [Baseline #7: TF-IDF and cosine similarity](#Baseline-#7:-TF-IDF-embeddings-+-cosine-similarity-(TF-IDF-+-cosine))\n",
    "    * [Baseline #8: Doc2vec and cosine similarity](#Baseline-#8:-Doc2vec-embeddings-+-cosine-similarity-(Doc2vec-+-cosine))\n",
    "\n",
    "* [Comparison of Baseline Models](#Comparison-of-Baseline-Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Packages\n",
    "import sys\n",
    "import os\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.spatial import distance\n",
    "import gensim\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load STS Benchmark Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we utilize the [STS Benchmark dataset](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark#STS_benchmark_dataset_and_companion_dataset) which contains a selection of English datasets that were used in Semantic Textual Similarity (STS) tasks 2012-2017. The datasets include text from image captions, news headlines, and user forums. The dataset contains 8,628 sentence pairs with a human-labeled integer representing the sentences' similarity (ranging from 0, for no meaning overlap, to 5, meaning equivalence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../../\")  ## set the environment path\n",
    "BASE_DATA_PATH = \"../../../data\"\n",
    "\n",
    "from utils_nlp.dataset.preprocess import to_lowercase, to_spacy_tokens\n",
    "from utils_nlp.dataset import stsbenchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Produce a pandas dataframe for the training and test sets\n",
    "stsTrain = stsbenchmark.load_pandas_df(BASE_DATA_PATH, file_split=\"train\")\n",
    "stsTest = stsbenchmark.load_pandas_df(BASE_DATA_PATH, file_split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 5749 sentences\n",
      "Testing set has 1379 sentences\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training set has {len(stsTrain)} sentences\")\n",
    "print(f\"Testing set has {len(stsTest)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.500</td>\n",
       "      <td>A girl is styling her hair.</td>\n",
       "      <td>A girl is brushing her hair.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.600</td>\n",
       "      <td>A group of men play soccer on the beach.</td>\n",
       "      <td>A group of boys are playing soccer on the beach.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.000</td>\n",
       "      <td>One woman is measuring another woman's ankle.</td>\n",
       "      <td>A woman measures another woman's ankle.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.200</td>\n",
       "      <td>A man is cutting up a cucumber.</td>\n",
       "      <td>A man is slicing a cucumber.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.500</td>\n",
       "      <td>A man is playing a harp.</td>\n",
       "      <td>A man is playing a keyboard.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.800</td>\n",
       "      <td>A woman is cutting onions.</td>\n",
       "      <td>A woman is cutting tofu.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.500</td>\n",
       "      <td>A man is riding an electric bicycle.</td>\n",
       "      <td>A man is riding a bicycle.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.200</td>\n",
       "      <td>A man is playing the drums.</td>\n",
       "      <td>A man is playing the guitar.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.200</td>\n",
       "      <td>A man is playing guitar.</td>\n",
       "      <td>A lady is playing the guitar.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.714</td>\n",
       "      <td>A man is playing a guitar.</td>\n",
       "      <td>A man is playing a trumpet.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                      sentence1  \\\n",
       "0  2.500                    A girl is styling her hair.   \n",
       "1  3.600       A group of men play soccer on the beach.   \n",
       "2  5.000  One woman is measuring another woman's ankle.   \n",
       "3  4.200                A man is cutting up a cucumber.   \n",
       "4  1.500                       A man is playing a harp.   \n",
       "5  1.800                     A woman is cutting onions.   \n",
       "6  3.500           A man is riding an electric bicycle.   \n",
       "7  2.200                    A man is playing the drums.   \n",
       "8  2.200                       A man is playing guitar.   \n",
       "9  1.714                     A man is playing a guitar.   \n",
       "\n",
       "                                          sentence2  \n",
       "0                      A girl is brushing her hair.  \n",
       "1  A group of boys are playing soccer on the beach.  \n",
       "2           A woman measures another woman's ankle.  \n",
       "3                      A man is slicing a cucumber.  \n",
       "4                      A man is playing a keyboard.  \n",
       "5                          A woman is cutting tofu.  \n",
       "6                        A man is riding a bicycle.  \n",
       "7                      A man is playing the guitar.  \n",
       "8                     A lady is playing the guitar.  \n",
       "9                       A man is playing a trumpet.  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stsTest.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing / Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline models will expect that each sentence is represented by a list of **tokens**. Tokens are linguistic units like words, punctuation marks, numbers, etc. We'll use our util functions which utilize the spaCy package, a popular package for performing tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also common to remove high-frequency words which do not help distinguish one sentence from another, so called **stop words**. For example, \"the\", \"and\", \"a\", etc. are typical stop words although each tokenization package may differ in the words they consider to be stop words. We'll tokenize our corpus with and without stop words so that we can compare our methods with and without stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_nlp.dataset.preprocess import (\n",
    "    to_lowercase,\n",
    "    to_spacy_tokens,\n",
    "    rm_spacy_stopwords,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train preprocessing\n",
    "df_low = to_lowercase(stsTrain)  # covert all text to lowercase\n",
    "sts_tokenize = to_spacy_tokens(df_low)  # tokenize normally\n",
    "sts_train_stop = rm_spacy_stopwords(sts_tokenize)  # tokenize with removal of stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each row in our dataframe contains:  \n",
    "- The similarity score of the sentence pair\n",
    "- The 2 original sentences from our datasets  \n",
    "- A column for each sentence's tokenization with stop words  \n",
    "- A column for each sentence's tokenization without stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>sentence1_tokens</th>\n",
       "      <th>sentence2_tokens</th>\n",
       "      <th>sentence1_tokens_stop</th>\n",
       "      <th>sentence2_tokens_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.00</td>\n",
       "      <td>a plane is taking off.</td>\n",
       "      <td>an air plane is taking off.</td>\n",
       "      <td>[a, plane, is, taking, off, .]</td>\n",
       "      <td>[an, air, plane, is, taking, off, .]</td>\n",
       "      <td>[plane, taking, .]</td>\n",
       "      <td>[air, plane, taking, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.80</td>\n",
       "      <td>a man is playing a large flute.</td>\n",
       "      <td>a man is playing a flute.</td>\n",
       "      <td>[a, man, is, playing, a, large, flute, .]</td>\n",
       "      <td>[a, man, is, playing, a, flute, .]</td>\n",
       "      <td>[man, playing, large, flute, .]</td>\n",
       "      <td>[man, playing, flute, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.80</td>\n",
       "      <td>a man is spreading shreded cheese on a pizza.</td>\n",
       "      <td>a man is spreading shredded cheese on an uncoo...</td>\n",
       "      <td>[a, man, is, spreading, shreded, cheese, on, a...</td>\n",
       "      <td>[a, man, is, spreading, shredded, cheese, on, ...</td>\n",
       "      <td>[man, spreading, shreded, cheese, pizza, .]</td>\n",
       "      <td>[man, spreading, shredded, cheese, uncooked, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.60</td>\n",
       "      <td>three men are playing chess.</td>\n",
       "      <td>two men are playing chess.</td>\n",
       "      <td>[three, men, are, playing, chess, .]</td>\n",
       "      <td>[two, men, are, playing, chess, .]</td>\n",
       "      <td>[men, playing, chess, .]</td>\n",
       "      <td>[men, playing, chess, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.25</td>\n",
       "      <td>a man is playing the cello.</td>\n",
       "      <td>a man seated is playing the cello.</td>\n",
       "      <td>[a, man, is, playing, the, cello, .]</td>\n",
       "      <td>[a, man, seated, is, playing, the, cello, .]</td>\n",
       "      <td>[man, playing, cello, .]</td>\n",
       "      <td>[man, seated, playing, cello, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                      sentence1  \\\n",
       "0   5.00                         a plane is taking off.   \n",
       "1   3.80                a man is playing a large flute.   \n",
       "2   3.80  a man is spreading shreded cheese on a pizza.   \n",
       "3   2.60                   three men are playing chess.   \n",
       "4   4.25                    a man is playing the cello.   \n",
       "\n",
       "                                           sentence2  \\\n",
       "0                        an air plane is taking off.   \n",
       "1                          a man is playing a flute.   \n",
       "2  a man is spreading shredded cheese on an uncoo...   \n",
       "3                         two men are playing chess.   \n",
       "4                 a man seated is playing the cello.   \n",
       "\n",
       "                                    sentence1_tokens  \\\n",
       "0                     [a, plane, is, taking, off, .]   \n",
       "1          [a, man, is, playing, a, large, flute, .]   \n",
       "2  [a, man, is, spreading, shreded, cheese, on, a...   \n",
       "3               [three, men, are, playing, chess, .]   \n",
       "4               [a, man, is, playing, the, cello, .]   \n",
       "\n",
       "                                    sentence2_tokens  \\\n",
       "0               [an, air, plane, is, taking, off, .]   \n",
       "1                 [a, man, is, playing, a, flute, .]   \n",
       "2  [a, man, is, spreading, shredded, cheese, on, ...   \n",
       "3                 [two, men, are, playing, chess, .]   \n",
       "4       [a, man, seated, is, playing, the, cello, .]   \n",
       "\n",
       "                         sentence1_tokens_stop  \\\n",
       "0                           [plane, taking, .]   \n",
       "1              [man, playing, large, flute, .]   \n",
       "2  [man, spreading, shreded, cheese, pizza, .]   \n",
       "3                     [men, playing, chess, .]   \n",
       "4                     [man, playing, cello, .]   \n",
       "\n",
       "                               sentence2_tokens_stop  \n",
       "0                            [air, plane, taking, .]  \n",
       "1                           [man, playing, flute, .]  \n",
       "2  [man, spreading, shredded, cheese, uncooked, p...  \n",
       "3                           [men, playing, chess, .]  \n",
       "4                   [man, seated, playing, cello, .]  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts_train_stop.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat process to perform preprocessing for test set\n",
    "df_low = to_lowercase(stsTest)\n",
    "sts_tokenize = to_spacy_tokens(df_low)\n",
    "sts_test_stop = rm_spacy_stopwords(sts_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Frequency Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the baseline models we'll explore will require calculation of how frequently a word appears in the sentences of our corpus. In this preprocessing step, we iterate through the sentences in our training set, counting the number of sentences that contain each word. There are other ways to produce this calculation, including pulling larger datasets from the web (like Wikipedia data) and calculating the frequency on that data. Note that \"document\" refers to some larger chunk of multiple tokens/words. In our case, our documents will actually be individual sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_frequency(df):\n",
    "    \"\"\"Iterate through all sentences in dataframe df and create dictionary mapping tokens to the number of sentences\n",
    "    they appear in in our corpus\n",
    "    \n",
    "    Args:\n",
    "        df (pandas dataframe): dataframe of sentence pairs with their similarity scores\n",
    "        \n",
    "    Returns:\n",
    "        document_frequency_dict (dictionary): mapping from tokens to number of sentences they appear in\n",
    "        n (int): number of sentences in the corpus\n",
    "    \"\"\"\n",
    "    document_frequency_dict = {}\n",
    "    sentences = df[\"sentence1_tokens\"].append(df[\"sentence2_tokens\"])\n",
    "\n",
    "    for s in sentences:\n",
    "        for token in set(s):\n",
    "            if token in document_frequency_dict:\n",
    "                document_frequency_dict[token] += 1\n",
    "            else:\n",
    "                document_frequency_dict[token] = 1\n",
    "    n = len(sentences)\n",
    "    return document_frequency_dict, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we need to calculate these values on our training set so that we don't \"peek at\" our test set until test time\n",
    "document_frequencies, num_documents = get_document_frequency(sts_train_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11498"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we consider each of the baseline models, we'll save all model predictions in a dictionary and will evaluate the results at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline #1: word2vec embeddings + cosine similarity (word2vec + cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This baseline first constructs word embeddings using word2vec. Once we have a word embedding (vector) for each word in the sentence, we calculate an embedding for the full sentence by taking the (weighted) average of all the word embeddings. The weights will be calculated using TF-IDF. Lastly, in order to compare the two sentence embeddings we use the cosine similarity metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Word2Vec?\n",
    "Word2vec is a predictive model for learning word embeddings from text. Word embeddings are learned such that words that share common contexts in the corpus will be close together in the vector space. There are two different model architectures that can be used to produce word2vec embeddings: continuous bag-of-words (CBOW) or continuous skip-gram. The former uses a window of surrounding words (the \"context\") to predict the current word and the latter uses the current word to predict the surrounding context words. See this [tutorial](https://www.guru99.com/word-embedding-word2vec.html#3) on word2vec for more detailed background on the model.\n",
    "\n",
    "For our purposes, we use pretrained word2vec word embeddings. These embeddings were trained on a Google News corpus and provide 300-dimensional embeddings (vectors) for 3 million English words. See this link for the original source (https://code.google.com/archive/p/word2vec/) and see the code below to load these word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.65GB [01:00, 27.3MB/s]                                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.keyedvectors.Word2VecKeyedVectors'>\n"
     ]
    }
   ],
   "source": [
    "from utils_nlp.pretrained_embeddings import word2vec\n",
    "\n",
    "word2vec_model = word2vec.load_pretrained_vectors(dir_path=BASE_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is TF-IDF?\n",
    "\n",
    "TF-IDF or term frequency-inverse document frequency is a weighting scheme intended to measure how important a word is to the document (or sentence in our case) within the broader corpus (our dataset). The weight \"increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus\" ([tutorial link](http://www.tfidf.com/)). When we're averaging together many different word vectors to get a sentence embedding, it makes sense to give stronger weight to words that are more distinct relative to the corpus and that have a high frequency in the sentence. The TF-IDF weights capture this intution, with the weight increasing as term frequency increases and/or as the inverse document frequency increases.\n",
    "\n",
    "For a term $t$ in sentence $s$ in corpus $c$, then the TF-IDF weight is \n",
    "$$w_{t,s} = TF_{t,s} * \\log{\\frac{N}{df_t}}$$\n",
    "where:  \n",
    "$TF_{t,s}$ = the number of times term $t$ appears in sentence $s$  \n",
    "$df_t$ = the number of sentences containing term $t$  \n",
    "$N$ = the size of the corpus.  \n",
    "\n",
    "In these baselines, we calculate the TF-IDF weighted average of all the word embeddings. The code below implements this weighted average given a list of tokens and an embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_sentence_embedding(tokens, embedding_model):\n",
    "    \"\"\"Calculate TF-IDF weighted average embedding for a sentence\n",
    "    \n",
    "    Args:\n",
    "        tokens (list): list of tokens in a sentence\n",
    "        embedding_model (gensim model): model to use for word embedding (word2vec, glove, fastText, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        list: vector representing the sentence\n",
    "    \"\"\"\n",
    "    # throw away tokens that are not in embeddings model\n",
    "    tokens = [i for i in tokens if i in embedding_model]\n",
    "\n",
    "    if len(tokens) == 0:\n",
    "        return []\n",
    "\n",
    "    # We will weight by TF-IDF:\n",
    "    # For the TF part: # of times term appears / total terms in sentence\n",
    "    count = Counter(tokens)\n",
    "    token_list = list(count)\n",
    "    term_frequency = [count[i] / len(tokens) for i in token_list]\n",
    "\n",
    "    # Now for the IDF part: LOG(# documents / # documents with term in it)\n",
    "    inv_doc_frequency = [\n",
    "        math.log(num_documents / (document_frequencies.get(i, 0) + 1)) for i in count\n",
    "    ]\n",
    "\n",
    "    # Put the TF-IDF together and produce the weighted average of vector embeddings\n",
    "    word_embeddings = [embedding_model[token] for token in token_list]\n",
    "    weights = [term_frequency[i] * inv_doc_frequency[i] for i in range(len(token_list))]\n",
    "    return list(np.average(word_embeddings, weights=weights, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Cosine Similarity?\n",
    "\n",
    "Cosine similarity is a common similarity metric between vectors. Intuitively it measures the cosine of the angle between any two vectors. With vectors $a$ and $b$, the cosine similarity is: cosine similarity($a$,$b$) = $\\frac{\\vec{a} \\cdot \\vec{b} }{||\\vec{a}|| ||\\vec{b}||}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(embedding1, embedding2):\n",
    "    \"\"\"Calculate cosine similarity between two embedding vectors\n",
    "    \n",
    "    Args:\n",
    "        embedding1 (list): embedding for the first sentence\n",
    "        embedding2 (list): embedding for the second sentence\n",
    "    \n",
    "    Returns:\n",
    "        list: cosine similarity value between the two embeddings\n",
    "    \"\"\"\n",
    "    # distance.cosine calculates cosine DISTANCE, so take 1 - distance to get cosine similarity\n",
    "    cosine_similarity = 1 - distance.cosine(embedding1, embedding2)\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Sentence Similarity Predictions for Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we calculate predictions for each of sentence pairs found in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_embedding_cosine_similarity(df, embedding_model, rm_stopwords=False):\n",
    "    \"\"\"Calculate the cosine similarity between TF-IDF weighted averaged embeddings\n",
    "    \n",
    "    Args:\n",
    "        df (pandas dataframe): dataframe as provided by the nlp_utils\n",
    "        embedding_model (gensim model): word embedding model\n",
    "        rm_stopwords (bool): whether to use stop words (True) or remove them (False)\n",
    "    \n",
    "    Returns:\n",
    "        list: predicted values for sentence similarity of test set examples\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    if rm_stopwords:\n",
    "        tokenized_sentences = zip(\n",
    "            df[\"sentence1_tokens_stop\"], df[\"sentence2_tokens_stop\"]\n",
    "        )\n",
    "    else:\n",
    "        tokenized_sentences = zip(df[\"sentence1_tokens\"], df[\"sentence2_tokens\"])\n",
    "\n",
    "\n",
    "    for (sentence1, sentence2) in tokenized_sentences:\n",
    "        embedding1 = average_sentence_embedding(sentence1, embedding_model)\n",
    "        embedding2 = average_sentence_embedding(sentence2, embedding_model)\n",
    "        if embedding1 == [] or embedding2 == []:\n",
    "            predictions.append(0)\n",
    "        else:\n",
    "            predictions.append(calculate_cosine_similarity(embedding1, embedding2))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions using average word2vec embeddings both with and without stop words\n",
    "baselines[\"Word2vec Cosine\"] = average_word_embedding_cosine_similarity(\n",
    "    sts_test_stop, word2vec_model, rm_stopwords=True\n",
    ")\n",
    "baselines[\"Word2vec Cosine with Stop Words\"] = average_word_embedding_cosine_similarity(\n",
    "    sts_test_stop, word2vec_model, rm_stopwords=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline #2: word2vec embeddings + Word Mover's Distance (word2vec + WMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This baseline first constructs word embeddings using word2vec (for an introduction to word2vec, see [Background on Word2Vec](#What-is-Word2Vec?)). Then all the word embeddings are used to calculate sentence similarity using the word mover's distance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Word Mover's Distance (WMD)?\n",
    "Word Mover's Distance (WMD) is a metric that \"adapts the earth mover’s distance to the space of documents: the distance between two texts is given by the total amount of “mass” needed to move the words from one side into the other, multiplied by the distance the words need to move.\" We'll utilize word2vec's implementation of word mover's distance. See this [blog](http://vene.ro/blog/word-movers-distance-in-python.html) for additional information about this similarity measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Sentence Similarity Predictions for Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we calculate predictions for each of sentence pairs found in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding_WMD(df, embedding_model, rm_stopwords=False):\n",
    "    \"\"\"Calculate Word Mover's Distance between two sentences using embeddings\n",
    "    \n",
    "    Args:\n",
    "        df (pandas dataframe): dataframe as provided by the nlp_utils\n",
    "        embedding_model (gensim model): word embedding model\n",
    "        rm_stopwords (bool): whether to use stop words (True) or remove them (False)\n",
    "    \n",
    "    Returns:\n",
    "        list: predicted values for sentence similarity of test set examples\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    if rm_stopwords:\n",
    "        tokenized_sentences = zip(\n",
    "            df[\"sentence1_tokens_stop\"], df[\"sentence2_tokens_stop\"]\n",
    "        )\n",
    "    else:\n",
    "        tokenized_sentences = zip(df[\"sentence1_tokens\"], df[\"sentence2_tokens\"])\n",
    "\n",
    "    for (sentence1, sentence2) in tokenized_sentences:\n",
    "        # throw away tokens that are not in embeddings model\n",
    "        tokens1 = [i for i in sentence1 if i in embedding_model]\n",
    "        tokens2 = [i for i in sentence2 if i in embedding_model]\n",
    "        if tokens1 == [] or tokens2 == []:\n",
    "            predictions.append(0)\n",
    "        else:\n",
    "            # wmdistance takes the raw tokens and performs the word2vec embedding itself\n",
    "            predictions.append(-embedding_model.wmdistance(tokens1, tokens2))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions using word2vec embeddings and WMD both with and without stop words\n",
    "baselines[\"Word2vec WMD\"] = word_embedding_WMD(sts_test_stop, word2vec_model, rm_stopwords=True)\n",
    "baselines[\"Word2vec WMD with Stop Words\"] = word_embedding_WMD(\n",
    "    sts_test_stop, word2vec_model, rm_stopwords=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline #3: GloVe embeddings + cosine similarity (GloVe + cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This baseline first constructs word embeddings using GloVE. Once we have a word embedding (vector) for each word in the sentence, we calculate an embedding for the full sentence by taking the (weighted) average of all the word embeddings. The weights will be calculated using TF-IDF. Lastly, in order to compare the two sentence embeddings we use the cosine similarity metric (for an introduction to the cosine similarity metric, see [Background on Cosine Similarity](#What-is-Cosine-Similarity?)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is GloVe?\n",
    "GloVe is an unsupervised algorithm for obtaining word embeddings. Training occurs on word-word co-occurance statistics with the objective of learning word embeddings such that the dot product of two word's embeddings is equal to the word's probability of co-occurance. See this [tutorial](https://nlp.stanford.edu/projects/glove/) on GloVe for more detailed background on the model. For our purposes, we use pretrained GloVe word embeddings (glove.840B.300d.zip which can be downloaded from above link). These embeddings were trained on Common Crawl data and provide 300-dimensional embeddings (vectors) for 2.2 million English words. Below is the code to load in the GloVe embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "tmp_file = \"glove.840B.300d.w2v.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove.840B.300d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-0869262143e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# we need to download the GLoVe file and convert it to word2vec format, this takes a bit of time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mPATH_TO_GLOVE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"glove.840B.300d.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mglove2word2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH_TO_GLOVE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmp_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\azureml\\lib\\site-packages\\gensim\\scripts\\glove2word2vec.py\u001b[0m in \u001b[0;36mglove2word2vec\u001b[1;34m(glove_input_file, word2vec_output_file)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \"\"\"\n\u001b[1;32m--> 104\u001b[1;33m     \u001b[0mnum_lines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_glove_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_input_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"converting %i vectors from %s to %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_lines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglove_input_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2vec_output_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2vec_output_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\azureml\\lib\\site-packages\\gensim\\scripts\\glove2word2vec.py\u001b[0m in \u001b[0;36mget_glove_info\u001b[1;34m(glove_file_name)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \"\"\"\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_file_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[0mnum_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_file_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\azureml\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[1;34m(uri, mode, **kw)\u001b[0m\n\u001b[0;32m    398\u001b[0m             \u001b[0mtransport_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_ext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_extension\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransport_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mscrubbed_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\azureml\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m         \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m     )\n\u001b[0;32m    302\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\azureml\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, ignore_ext, buffering, encoding, errors)\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 459\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    460\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mopen_kwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.840B.300d.txt'"
     ]
    }
   ],
   "source": [
    "# we need to download the GLoVe file and convert it to word2vec format, this takes a bit of time\n",
    "PATH_TO_GLOVE = os.path.expanduser(\"glove.840B.300d.txt\")\n",
    "glove2word2vec(PATH_TO_GLOVE, tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the above cell has been run once, just need to use this command to load the embeddings\n",
    "glove = gensim.models.KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions using glove embeddings and cosine similarity both with and without stop words\n",
    "baselines[\"GLoVe Cosine\"] = average_word_embedding_cosine_similarity(\n",
    "    sts_test_stop, glove, rm_stopwords=True\n",
    ")\n",
    "baselines[\"GLoVe Cosine with Stop Words\"] = average_word_embedding_cosine_similarity(\n",
    "    sts_test_stop, glove, rm_stopwords=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline #4: GloVe embeddings + Word Mover's Distance (GloVe + WMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This baseline first constructs word embeddings using GloVe (for an introduction on GloVe, see [Background on GloVe](#What-is-GloVe?)). Then all the word embeddings are used to calculate sentence similarity using the word mover's distance (for an introduction to WMD, see [Background on Word Mover's Distance](#What-is-Word-Mover's-Distance-(WMD)?)).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions using glove embeddings and WMD both with and without stop words\n",
    "baselines[\"GLoVe WMD\"] = word_embedding_WMD(sts_test_stop, glove, rm_stopwords=True)\n",
    "baselines[\"GLoVe WMD with Stop Words\"] = word_embedding_WMD(\n",
    "    sts_test_stop, glove, rm_stopwords=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline #5: fastText embeddings + cosine similarity (fastText + cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This baseline first constructs word embeddings using fastText. Once we have a word embedding (vector) for each word in the sentence, we calculate an embedding for the full sentence by taking the (weighted) average of all the word embeddings. The weights will be calculated using TF-IDF. Lastly, in order to compare the two sentence embeddings we use the cosine similarity metric (for an introduction to the cosine similarity metric, see [Background on Cosine Similarity](#What-is-Cosine-Similarity?)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is fastText?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastText is an unsupervised algorithm created by Facebook Research for efficiently learning word embeddings. fastText is significantly different than word2vec or GloVe in that these two algorithms we saw earlier treat each word as the smallest possible unit to find an embedding for. Conversely, fastText assumes that words are formed by an n-gram of characters (i.e. 2-grams of the word \"language\" would be {la, an, ng, gu, ua, ag, ge}). The embedding for a word is then composed of the sum of these character n-grams. This has advantages when finding word embeddings for rare words and words not present in the dictionary, as these words can still be broken down into character n-grams. Typically, for smaller datasets, fastText performs better than word2vec or GloVe. See this [tutorial](https://fasttext.cc/docs/en/unsupervised-tutorial.html) on fastText for more detail. We will use the pretrained word embeddings for the English language (wiki.en.bin; these embeddings as well as embeddings for 156 other languages can be found at https://fasttext.cc/docs/en/english-vectors.html). These are 300-dimensional embeddings (vectors) trained on Wikipedia data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_model = gensim.models.fasttext.load_facebook_model(\"wiki.en\\wiki.en.bin\")\n",
    "fastText = fastText_model.wv  # get word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions using fastText embeddings and cosine similarity both with and without stop words\n",
    "baselines[\"fastText Cosine\"] = average_word_embedding_cosine_similarity(\n",
    "    sts_test_stop, fastText, rm_stopwords=True\n",
    ")\n",
    "baselines[\"fastText Cosine with Stop Words\"] = average_word_embedding_cosine_similarity(\n",
    "    sts_test_stop, fastText, rm_stopwords=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline #6: fastText embeddings + Word Mover's Distance (fastText + WMD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions using fastText embeddings and WMD both with and without stop words\n",
    "baselines[\"fastText WMD\"] = word_embedding_WMD(sts_test_stop, fastText, rm_stopwords=True)\n",
    "baselines[\"fastText WMD with Stop Words\"] = word_embedding_WMD(\n",
    "    sts_test_stop, fastText, rm_stopwords=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline #7: TF-IDF embeddings + cosine similarity (TF-IDF + cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This baseline first constructs a document embedding based on bag of words with TF-IDF weighting (for an introduction to TF-IDF, see [Background on TF-IDF](#What-is-TF-IDF?). Then we apply cosine similarity between the two embeddings in the sentence pair (for an introduction to the cosine similarity metric, see [Background on Cosine Similarity](#What-is-Cosine-Similarity?))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "The most basic approach for document embeddings is called Bag-of-Words. This method first determines the vocabulary across the entire corpus and then, for each document, creates a vector containing the number of times each vocabulary word appeared in the given document. These vectors are obviously very sparse and typical bag of words implementations ignore terms whose document frequency is less than some threshold in order to reduce sparsity. We also often ignore stop words as they add little semantic information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_cosine_similarity(df, rm_stopwords=False):\n",
    "    \"\"\"Calculate cosine similarity between TF-IDF document embeddings\n",
    "    \n",
    "    Args:\n",
    "        df (pandas dataframe): dataframe as provided by the nlp_utils\n",
    "        rm_stopwords (bool): whether to remove stop words or not\n",
    "    \n",
    "    Returns:\n",
    "        list: predicted values for sentence similarity of test set examples\n",
    "    \"\"\"\n",
    "    stop_word_param = 'english' if rm_stopwords else None\n",
    "    \n",
    "    tf = TfidfVectorizer(\n",
    "        input=\"content\",\n",
    "        analyzer=\"word\",\n",
    "        min_df=0,\n",
    "        stop_words=stop_word_param,\n",
    "        sublinear_tf=True,\n",
    "    )\n",
    "\n",
    "    all_sentences =  df[[\"sentence1\", \"sentence2\"]]\n",
    "    corpus = all_sentences.values.flatten().tolist()\n",
    "    tfidf_matrix = np.array(tf.fit_transform(corpus).todense())\n",
    "    \n",
    "    df['sentence1_tfidf'] = df.apply(lambda x: tfidf_matrix[2*x.name,:], axis=1)\n",
    "    df['sentence2_tfidf'] = df.apply(lambda x: tfidf_matrix[2*x.name+1,:], axis=1)\n",
    "    df['predictions'] = df.apply(lambda x: calculate_cosine_similarity(x.sentence1_tfidf, x.sentence2_tfidf) if \n",
    "                                 (sum(x.sentence1_tfidf) != 0 and sum(x.sentence2_tfidf) != 0) else 0,axis=1)\n",
    "    return df['predictions'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  1  4\n",
       "1  2  5\n",
       "2  3  6"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = pd.DataFrame({'a':[1,2,3],'b':[4,5,6]})\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>index</th>\n",
       "      <th>sent1</th>\n",
       "      <th>sent2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b  index  sent1  sent2\n",
       "0  1  4      0      0      1\n",
       "1  2  5      1      2      3\n",
       "2  3  6      2      4      5"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex['sent1'] = ex.apply(lambda x: 2*x.name,axis=1)\n",
    "ex['sent2'] = ex.apply(lambda x: 2*x.name+1,axis=1)\n",
    "\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "baselines[\"TF-IDF Cosine\"] = tfidf_cosine_similarity(sts_test_stop, rm_stopwords=True)\n",
    "baselines[\"TF-IDF Cosine with Stop Words\"] = tfidf_cosine_similarity(\n",
    "    sts_test_stop, rm_stopwords=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5154669367957403,\n",
       " 0.6238376709093325,\n",
       " 0.5983062335986293,\n",
       " 0.7102615298532333,\n",
       " 0.3351548713228747,\n",
       " 0.4586765374374362,\n",
       " 0.7453648441043782,\n",
       " 0.4332143047353778,\n",
       " 0.6274081602186135,\n",
       " 0.4102968379263213,\n",
       " 0.4102968379263213,\n",
       " 0.5253589511249759,\n",
       " 0.17651528643337078,\n",
       " 0.5499522389851709,\n",
       " 0.3787193514643298,\n",
       " 0.4332143047353778,\n",
       " 0.5423224039397149,\n",
       " 0.7519050657939645,\n",
       " 0.30617844695350493,\n",
       " 0.12920385409392754,\n",
       " 0.5115388235096275,\n",
       " 0.12571551261385527,\n",
       " 0.671208418075113,\n",
       " 0.6147994882540094,\n",
       " 0.3085863437907099,\n",
       " 0.8665043113349088,\n",
       " 0.3499638723495143,\n",
       " 0.4485303934478104,\n",
       " 0.14412070093774276,\n",
       " 0.7223992863241857,\n",
       " 0.42977927537233007,\n",
       " 0.6687393429045153,\n",
       " 0.09883942492880626,\n",
       " 0.8006601194016243,\n",
       " 0.3915728034097792,\n",
       " 0.12959796544910263,\n",
       " 0.07074855491798504,\n",
       " 0.24392238095706376,\n",
       " 0.5797213695434924,\n",
       " 0.1455830629990571,\n",
       " 0.5774415169951077,\n",
       " 0.7562038442807758,\n",
       " 0.7519050657939645,\n",
       " 0.14623990642389495,\n",
       " 0.271411211816421,\n",
       " 0.10679246383512553,\n",
       " 0.12436018172562091,\n",
       " 0.24282578702512114,\n",
       " 0.649298946993787,\n",
       " 0.48951256029075396,\n",
       " 0.22269264312418413,\n",
       " 0.13418067046494309,\n",
       " 0.1475892907985462,\n",
       " 0.11919014738377365,\n",
       " 0.11758015372101038,\n",
       " 0.0,\n",
       " 0.5851543274311135,\n",
       " 0.1308592565135699,\n",
       " 0.7699878532034286,\n",
       " 0.3204871058453277,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.5113876993851364,\n",
       " 0.11301699334082715,\n",
       " 0.4844516290250571,\n",
       " 0.08881150230557444,\n",
       " 0.0,\n",
       " 0.5991676539776296,\n",
       " 0.0,\n",
       " 0.42156705698583274,\n",
       " 0.5269248364657431,\n",
       " 0.3655237377185544,\n",
       " 0.0,\n",
       " 0.6344119096586092,\n",
       " 0.4153702218957458,\n",
       " 0.0,\n",
       " 0.17659756230874368,\n",
       " 0.0,\n",
       " 0.5773711547137133,\n",
       " 0.3400513696315266,\n",
       " 0.0,\n",
       " 0.1831492472287829,\n",
       " 0.23854126483382898,\n",
       " 0.1511207255960434,\n",
       " 0.14677017202961828,\n",
       " 0.10406317742927851,\n",
       " 0.6201409333213322,\n",
       " 0.34867041329567816,\n",
       " 0.5558631416993667,\n",
       " 0.23888620151049622,\n",
       " 0.0,\n",
       " 0.8132717568057986,\n",
       " 0.0,\n",
       " 0.5131764335105033,\n",
       " 0.08112645666290907,\n",
       " 0.0,\n",
       " 0.40984286310643414,\n",
       " 0.0,\n",
       " 0.15163144931355121,\n",
       " 0.11562148003468309,\n",
       " 0.0,\n",
       " 0.05819882110600594,\n",
       " 0.36864716569966793,\n",
       " 0.5332166574305218,\n",
       " 0.361604740013435,\n",
       " 0.37159316532516296,\n",
       " 0.0,\n",
       " 0.08889801633124483,\n",
       " 0.06783882062422586,\n",
       " 0.39587926385535976,\n",
       " 0.21373328317022044,\n",
       " 0.3210726805003803,\n",
       " 0.27988207621810623,\n",
       " 0.6760684435794222,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.38097658851652527,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.06861096018409485,\n",
       " 0.04908910946944067,\n",
       " 0.0708011632071649,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.7837591866549787,\n",
       " 0.5377343851873254,\n",
       " 0.4889182045146514,\n",
       " 0.6703932250207224,\n",
       " 0.7241898046881592,\n",
       " 0.5771623250458925,\n",
       " 0.6216402101600226,\n",
       " 0.6823235058855647,\n",
       " 0.3280161588066738,\n",
       " 0.6918009620390254,\n",
       " 0.35865740721179473,\n",
       " 0.4172115816761691,\n",
       " 0.921908870714373,\n",
       " 0.5586504267825702,\n",
       " 0.7531542457363941,\n",
       " 0.4663577643586986,\n",
       " 0.8244339823877357,\n",
       " 0.47839637812100944,\n",
       " 0.3208083132446702,\n",
       " 0.4869969531085441,\n",
       " 0.3226097457268292,\n",
       " 0.5286609852613486,\n",
       " 0.17559289282597357,\n",
       " 0.34711853027470263,\n",
       " 0.4485303934478104,\n",
       " 0.70614165690267,\n",
       " 0.7519050657939645,\n",
       " 0.45922622756177234,\n",
       " 0.8126869583086342,\n",
       " 0.2905028770893301,\n",
       " 0.16895547254574517,\n",
       " 0.5717826716630433,\n",
       " 0.18344969420537105,\n",
       " 0.6881903625249085,\n",
       " 0.3744635997611292,\n",
       " 0.4038766428917129,\n",
       " 0.7629887778411776,\n",
       " 0.7075593623565637,\n",
       " 0.7211044894623567,\n",
       " 0.37993813876579297,\n",
       " 0.9578069784773438,\n",
       " 0.32895005819109835,\n",
       " 0.5233733505459711,\n",
       " 0.4332143047353778,\n",
       " 0.0,\n",
       " 0.27231427111936635,\n",
       " 0.5751553426708501,\n",
       " 0.299049421036619,\n",
       " 0.5089477348014251,\n",
       " 0.5777296110323978,\n",
       " 0.39476799484467184,\n",
       " 0.14704330739625027,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3070877471932829,\n",
       " 0.17052490412044174,\n",
       " 0.0783003764285174,\n",
       " 0.17186157327119655,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.14661451413971538,\n",
       " 0.08753407803623192,\n",
       " 0.22360812652997764,\n",
       " 0.5845971152319437,\n",
       " 0.4573533893034656,\n",
       " 0.1403723024049901,\n",
       " 0.36638933237713944,\n",
       " 0.6169097809414048,\n",
       " 0.47834650935596745,\n",
       " 0.5284223205808826,\n",
       " 0.07995225509440962,\n",
       " 0.0,\n",
       " 0.3071117184213411,\n",
       " 0.1326671474247091,\n",
       " 0.0906988696914991,\n",
       " 0.0,\n",
       " 0.06982225239298134,\n",
       " 0.06786341202563717,\n",
       " 0.1505053792432255,\n",
       " 0.5635780904316643,\n",
       " 0.0,\n",
       " 0.2621572974398185,\n",
       " 0.25308403507612054,\n",
       " 0.3221668821455286,\n",
       " 0.0629899640099777,\n",
       " 0.07072985167756685,\n",
       " 0.09158692018067738,\n",
       " 0.0,\n",
       " 0.06961382750938006,\n",
       " 0.0,\n",
       " 0.3991826870741608,\n",
       " 0.6258373248691018,\n",
       " 0.2024992428862169,\n",
       " 0.0934248056671858,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3383416797152323,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.25632764621403736,\n",
       " 0.26348364914109546,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4379521167311877,\n",
       " 0.41132949820204534,\n",
       " 0.11847442885460868,\n",
       " 0.2971498956613732,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09102468047929912,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.37263554283623235,\n",
       " 0.04783693739223094,\n",
       " 0.5553870306731815,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.40532393907395625,\n",
       " 0.8206164606646708,\n",
       " 0.25666914949469033,\n",
       " 0.6033023126321279,\n",
       " 0.08887873998082596,\n",
       " 0.5282344063364891,\n",
       " 0.1614889734989262,\n",
       " 0.6110703218466237,\n",
       " 0.41727537138938575,\n",
       " 0.0,\n",
       " 0.7005010433277096,\n",
       " 0.5243114597876252,\n",
       " 0.45631060038613414,\n",
       " 0.9294161965740012,\n",
       " 0.48528251524517985,\n",
       " 0.24378563314988155,\n",
       " 0.05594338497520712,\n",
       " 0.4421746149008551,\n",
       " 0.2377048214538765,\n",
       " 0.6813063418947445,\n",
       " 0.5481628563278482,\n",
       " 0.07514754084864528,\n",
       " 0.15056863440934065,\n",
       " 0.682620961448915,\n",
       " 0.8187241290910362,\n",
       " 0.7149131968798541,\n",
       " 0.13706932564725627,\n",
       " 0.0,\n",
       " 0.6869397791046526,\n",
       " 0.4611872605191232,\n",
       " 0.0,\n",
       " 0.15202275949382327,\n",
       " 0.7829108581186621,\n",
       " 0.5305878283081257,\n",
       " 0.6603118873459244,\n",
       " 0.28904172336610134,\n",
       " 0.42732471901100355,\n",
       " 0.6043752491632897,\n",
       " 0.7109249444765121,\n",
       " 0.30480391163182174,\n",
       " 0.5698722318653697,\n",
       " 0.20549585132781023,\n",
       " 0.3177788370599429,\n",
       " 0.8614230258391126,\n",
       " 0.0,\n",
       " 0.17961306027344115,\n",
       " 0.2986599562526925,\n",
       " 0.540420082234669,\n",
       " 0.35607441781909666,\n",
       " 0.8002769658830887,\n",
       " 1.0,\n",
       " 0.5010692009498263,\n",
       " 0.6337345012267683,\n",
       " 0.8856793779921786,\n",
       " 0.11445707552032425,\n",
       " 0.4750840293001414,\n",
       " 0.7621116166681253,\n",
       " 0.45004691560501453,\n",
       " 0.8009532349029482,\n",
       " 0.4924676163676591,\n",
       " 0.4510285571092685,\n",
       " 0.14869900345027842,\n",
       " 0.5217159751008826,\n",
       " 0.6185803009187976,\n",
       " 0.4444468804695457,\n",
       " 0.6156453648502771,\n",
       " 0.14690996626271513,\n",
       " 0.660545645117679,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.7008561243606823,\n",
       " 0.33706833399580693,\n",
       " 0.7055523290156626,\n",
       " 0.0,\n",
       " 0.7634159743209235,\n",
       " 0.15529904417747142,\n",
       " 0.5300893560037124,\n",
       " 0.5351889030477158,\n",
       " 0.4575928100864117,\n",
       " 0.4846660450167073,\n",
       " 1.0,\n",
       " 0.6152809371757402,\n",
       " 0.8513420582525032,\n",
       " 0.7020435273072652,\n",
       " 0.5127017435109952,\n",
       " 0.12858246330763734,\n",
       " 0.8038360742821876,\n",
       " 0.5089461705324838,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.7891418878450985,\n",
       " 0.9128208385604433,\n",
       " 0.8636447810250142,\n",
       " 0.21607109097737087,\n",
       " 0.23764191211845587,\n",
       " 0.2575300659556512,\n",
       " 0.34468315200508814,\n",
       " 0.19525740855245732,\n",
       " 0.268269966041514,\n",
       " 0.13351770660464013,\n",
       " 0.35352177736162316,\n",
       " 0.8210427655645578,\n",
       " 0.5139485600912518,\n",
       " 0.7688518290170615,\n",
       " 0.38934200424583576,\n",
       " 0.17665958376757385,\n",
       " 0.11138334466238553,\n",
       " 0.559142631384032,\n",
       " 0.0,\n",
       " 0.3343619630920124,\n",
       " 0.8811380090163619,\n",
       " 0.5593623007859126,\n",
       " 0.5021808132755385,\n",
       " 0.6184246806279081,\n",
       " 0.7558520943647828,\n",
       " 0.0,\n",
       " 0.35749646392536905,\n",
       " 0.13513854690141414,\n",
       " 0.6356500175614648,\n",
       " 0.873180138156219,\n",
       " 0.6723659094433972,\n",
       " 0.7644141138879765,\n",
       " 0.16406654190325953,\n",
       " 0.49578015664063524,\n",
       " 0.24346092633110072,\n",
       " 0.6468713109969254,\n",
       " 0.44396916121555274,\n",
       " 0.12171382929777275,\n",
       " 0.3859715362687679,\n",
       " 0.4219185382179377,\n",
       " 0.8514539757495124,\n",
       " 0.6614020530955229,\n",
       " 0.3542799278463653,\n",
       " 0.1289559363516588,\n",
       " 0.44934071430844447,\n",
       " 0.3343502863931016,\n",
       " 0.8046166300068341,\n",
       " 0.1239826378737986,\n",
       " 0.45275869289234527,\n",
       " 0.1477172135589866,\n",
       " 1.0,\n",
       " 0.31793243661545034,\n",
       " 0.05158902041464353,\n",
       " 0.23509354099316182,\n",
       " 0.7497415311343633,\n",
       " 0.0,\n",
       " 0.09677819311728497,\n",
       " 0.6057599535781372,\n",
       " 0.5759253376853078,\n",
       " 0.4301930609628042,\n",
       " 0.19721669304753386,\n",
       " 0.20938378558781823,\n",
       " 0.4089957516485536,\n",
       " 0.6108464286754679,\n",
       " 0.4894615579955016,\n",
       " 0.5808175769123721,\n",
       " 0.0,\n",
       " 0.3126676960261622,\n",
       " 0.5481434369484262,\n",
       " 0.3524874860063153,\n",
       " 0.5360475210310603,\n",
       " 0.8788490095821638,\n",
       " 0.3442435750182118,\n",
       " 0.4503955705954288,\n",
       " 0.09029751535863328,\n",
       " 0.2012419993294795,\n",
       " 0.0,\n",
       " 0.9274114315997014,\n",
       " 0.5434147120672723,\n",
       " 0.8306753486272993,\n",
       " 0.4683515725501377,\n",
       " 0.18678545201034713,\n",
       " 1.0,\n",
       " 0.13425155055493154,\n",
       " 0.5880420237090649,\n",
       " 0.24168330407679195,\n",
       " 0.0,\n",
       " 0.522583588243413,\n",
       " 0.37341894102558615,\n",
       " 0.194127399548935,\n",
       " 0.2150669685194998,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.49356125237048354,\n",
       " 0.1272784783854516,\n",
       " 0.45966421954215453,\n",
       " 0.26882244630641616,\n",
       " 0.7202111764851974,\n",
       " 0.5806015635505841,\n",
       " 0.703370361888481,\n",
       " 0.42427983852282103,\n",
       " 0.3831859865292562,\n",
       " 0.0,\n",
       " 0.23560315672204657,\n",
       " 0.3172206457791551,\n",
       " 0.0,\n",
       " 0.219068883168148,\n",
       " 0.5102766121620987,\n",
       " 0.03038602830506265,\n",
       " 0.1430457736854147,\n",
       " 0.5502426372632354,\n",
       " 0.7084414111410967,\n",
       " 0.31631873777664365,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.37520019211582634,\n",
       " 0.18596151945689499,\n",
       " 0.6143897906634846,\n",
       " 0.11150528737291354,\n",
       " 0.4572159574549788,\n",
       " 0.05084109583994967,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.549302606412826,\n",
       " 0.4101521595181268,\n",
       " 0.47363544630879706,\n",
       " 0.37750434720141457,\n",
       " 0.599680729002257,\n",
       " 0.8484819322509444,\n",
       " 0.12219773475203377,\n",
       " 0.2502143229111057,\n",
       " 0.3089211179360021,\n",
       " 0.6066426362810541,\n",
       " 0.6744876985413576,\n",
       " 0.21440010969171996,\n",
       " 0.48194274236523627,\n",
       " 0.15520407102846667,\n",
       " 0.06874123653938524,\n",
       " 0.620786031355869,\n",
       " 0.1721030489923958,\n",
       " 0.5152269919664448,\n",
       " 0.12112787189985341,\n",
       " 0.3789096225584536,\n",
       " 0.0,\n",
       " 0.7866277343051556,\n",
       " 0.26783155476750453,\n",
       " 0.47330632367516756,\n",
       " 0.645621890323781,\n",
       " 0.34594800364765343,\n",
       " 1.0,\n",
       " 0.6492450786127035,\n",
       " 0.5235653744905541,\n",
       " 0.6061208909415156,\n",
       " 0.6573579523402031,\n",
       " 0.0,\n",
       " 0.5970584296887183,\n",
       " 0.13906849950465394,\n",
       " 0.4047053462934861,\n",
       " 0.12129364249563068,\n",
       " 0.7293432961325722,\n",
       " 0.5401439734854109,\n",
       " 0.4814045333954379,\n",
       " 0.12806683569310184,\n",
       " 0.8698657686207867,\n",
       " 0.3437417759174832,\n",
       " 0.8873617972961861,\n",
       " 0.5410742441943139,\n",
       " 0.2108311618785843,\n",
       " 0.5183677288116197,\n",
       " 0.0,\n",
       " 0.0916888376354833,\n",
       " 0.40785977246340366,\n",
       " 0.33966626473841144,\n",
       " 0.0,\n",
       " 0.5993027509253149,\n",
       " 0.0,\n",
       " 0.37819651917068864,\n",
       " 0.4156002552682925,\n",
       " 0.4641392660373229,\n",
       " 0.6519808199860605,\n",
       " 0.04494474848168728,\n",
       " 0.4647217406966456,\n",
       " 0.48784437303212824,\n",
       " 0.2726005238628906,\n",
       " 0.39480703612005263,\n",
       " 0.28644304493341854,\n",
       " 0.0,\n",
       " 0.07939562336249639,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0765777283572675,\n",
       " 0.3424986946520312,\n",
       " 0.2829883523026371,\n",
       " 0.0,\n",
       " 0.46390449626485497,\n",
       " 0.32273066113866045,\n",
       " 0.14148923053497942,\n",
       " 0.46312472417298534,\n",
       " 1.0,\n",
       " 0.699877476328809,\n",
       " 0.5508282738763569,\n",
       " 0.7472229403719705,\n",
       " 0.8106306659967933,\n",
       " 0.7436366752463702,\n",
       " 0.0,\n",
       " 0.5698741776860069,\n",
       " 0.40320562962610174,\n",
       " 0.6368366902435519,\n",
       " 0.6877295667132002,\n",
       " 0.17037115910344358,\n",
       " 0.6486701023855274,\n",
       " 0.8633709821844233,\n",
       " 0.16022601054849728,\n",
       " 0.640083045917081,\n",
       " 0.6697438573282328,\n",
       " 0.4487485770265547,\n",
       " 0.6719100338348823,\n",
       " 0.2685141361789576,\n",
       " 0.9028521136397718,\n",
       " 0.1622506352731724,\n",
       " 0.4539814651972416,\n",
       " 0.27654737254151907,\n",
       " 0.3844628500466558,\n",
       " 0.3599192969869678,\n",
       " 1.0,\n",
       " 0.31262747295222737,\n",
       " 0.0998819622034911,\n",
       " 0.5500318840350857,\n",
       " 0.31149975393979346,\n",
       " 0.4321177576181423,\n",
       " 0.38946637701000786,\n",
       " 0.5403237881465437,\n",
       " 0.6858924875847925,\n",
       " 0.8198141851102765,\n",
       " 0.8564742491936111,\n",
       " 0.5196833302356285,\n",
       " 0.35792820898885536,\n",
       " 0.39824779587910064,\n",
       " 0.2972380958131975,\n",
       " 0.719401296625734,\n",
       " 0.4803227150707702,\n",
       " 0.5125995715602804,\n",
       " 0.22681123638902323,\n",
       " 0.41301661519157196,\n",
       " 0.23254970822207288,\n",
       " 1.0,\n",
       " 0.3495836625459079,\n",
       " 0.3984112218773903,\n",
       " 0.2806722974408733,\n",
       " 0.866781590051423,\n",
       " 0.7681494330073158,\n",
       " 0.36325229737259024,\n",
       " 0.0,\n",
       " 0.7167472425025875,\n",
       " 0.5899089939551846,\n",
       " 0.0,\n",
       " 0.49115673778576774,\n",
       " 0.88362940787316,\n",
       " 0.6044395564617944,\n",
       " 0.0,\n",
       " 0.5085299951560898,\n",
       " 0.6756504169738741,\n",
       " 0.2582428942918765,\n",
       " 0.37900938595589095,\n",
       " 0.3909774315296721,\n",
       " 0.24954918104723167,\n",
       " 0.7102838327248886,\n",
       " 0.8691974050976067,\n",
       " 1.0,\n",
       " 0.6005255962140266,\n",
       " 0.5762709803364882,\n",
       " 0.25458143588052695,\n",
       " 0.42667737171879827,\n",
       " 0.4843225342213465,\n",
       " 0.4932812689599628,\n",
       " 0.2800079504661759,\n",
       " 0.4490563346904908,\n",
       " 0.6181593556723631,\n",
       " 0.03646100687435949,\n",
       " 0.7309912268417118,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.4035475345793893,\n",
       " 0.7840126745472233,\n",
       " 0.3954803616482563,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1591537194913175,\n",
       " 0.6387516302676169,\n",
       " 0.0,\n",
       " 0.38828176138779136,\n",
       " 0.0,\n",
       " 0.5397777149131365,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3968237818834943,\n",
       " 1.0,\n",
       " 0.2463884367894592,\n",
       " 0.37488257251367485,\n",
       " 0.40014008678755286,\n",
       " 0.3795872170838577,\n",
       " 0.3481696625036117,\n",
       " 1.0,\n",
       " 0.4532486284107131,\n",
       " 0.45124885781957236,\n",
       " 1.0,\n",
       " 0.28499747178485757,\n",
       " 0.47582331298981617,\n",
       " 0.8569892438797804,\n",
       " 0.5099943018262215,\n",
       " 0.0,\n",
       " 0.28016580350786,\n",
       " 0.8206164606646708,\n",
       " 0.676439262841274,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6284854481311963,\n",
       " 0.3783914617277526,\n",
       " 0.47754345753482585,\n",
       " 0.2243300379059756,\n",
       " 0.42310885240354024,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.7984425111203017,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.33411352598381483,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.4376974425852824,\n",
       " 1.0,\n",
       " 0.6641864829585565,\n",
       " 0.0,\n",
       " 0.4114352038515582,\n",
       " 0.2667636290931522,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.22915201570646015,\n",
       " 0.0,\n",
       " 0.6479035777820578,\n",
       " 0.3239238626021006,\n",
       " 0.39480292860784894,\n",
       " 0.2573377944004096,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.477762489986595,\n",
       " 0.8279624193673831,\n",
       " 0.5200494878273851,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.23367235288040167,\n",
       " 0.5970461949090197,\n",
       " 0.27374491608748885,\n",
       " 0.46303978762016773,\n",
       " 0.39773060094296664,\n",
       " 0.0,\n",
       " 0.8319693059732305,\n",
       " 0.41174823182562426,\n",
       " 0.709896977074388,\n",
       " 0.0,\n",
       " 0.5247823897386177,\n",
       " 0.6642672000619121,\n",
       " 0.18685860042264402,\n",
       " 0.6103548453839198,\n",
       " 0.0,\n",
       " 0.6296793435449906,\n",
       " 0.6666617252063537,\n",
       " 0.6850351236482123,\n",
       " 0.28502376330020907,\n",
       " 0.4351498686684909,\n",
       " 0.11218670184576496,\n",
       " 0.0,\n",
       " 0.38641719864521673,\n",
       " 0.7487690643478222,\n",
       " 0.7970850962959118,\n",
       " 0.37856165181789503,\n",
       " 0.7679402131473076,\n",
       " 0.6070492486988476,\n",
       " 0.35798642841884964,\n",
       " 0.19679168934593716,\n",
       " 0.0,\n",
       " 0.5332145072432587,\n",
       " 0.45611959388394785,\n",
       " 0.38524882472700495,\n",
       " 0.8047081251748675,\n",
       " 0.5492381930223644,\n",
       " 0.3567740839298559,\n",
       " 0.522620986362805,\n",
       " 0.59522523840903,\n",
       " 0.22484082300717556,\n",
       " 0.0,\n",
       " 0.19851216657351878,\n",
       " 0.26684870419901496,\n",
       " 0.2052299684689749,\n",
       " 0.0,\n",
       " 0.25063177573756357,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.16238590270650166,\n",
       " 0.21804980109424987,\n",
       " 0.40014008678755286,\n",
       " 0.3614000377087643,\n",
       " 0.3052284240319709,\n",
       " 1.0,\n",
       " 0.3809185295288766,\n",
       " 0.3713548812569696,\n",
       " 0.18707569297276838,\n",
       " 0.4477502307780068,\n",
       " 0.0,\n",
       " 0.7140966717635232,\n",
       " 0.4780702764219191,\n",
       " 0.38229031950949444,\n",
       " 0.6850351236482123,\n",
       " 0.0,\n",
       " 0.5947797094626923,\n",
       " 0.14401740271647645,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.5144459609080433,\n",
       " 0.611567382161153,\n",
       " 0.8584136093353725,\n",
       " 0.798892408043969,\n",
       " 0.6144257642143102,\n",
       " 0.4330346917739156,\n",
       " 0.3867014756245677,\n",
       " 0.0,\n",
       " 0.4976791325378406,\n",
       " 0.2258005366789897,\n",
       " 0.43585115599310564,\n",
       " 0.0,\n",
       " 0.4848911314386981,\n",
       " 0.5126680056301495,\n",
       " 0.0,\n",
       " 0.2373504569329732,\n",
       " 0.5424156578876764,\n",
       " 0.390342271686988,\n",
       " 0.5166691886105966,\n",
       " 0.5608506275623061,\n",
       " 0.8244404315451992,\n",
       " 0.7632478718966247,\n",
       " 0.0,\n",
       " 0.29137460642559,\n",
       " 0.29257426918802265,\n",
       " 0.7019612334107046,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.275953331199847,\n",
       " 0.6852570986732491,\n",
       " 0.4520288550570899,\n",
       " 0.5608904134601668,\n",
       " 0.6074888258058404,\n",
       " 0.48358504032722727,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.2914801861522027,\n",
       " 0.28615316182957407,\n",
       " 0.0,\n",
       " 0.21766798294887157,\n",
       " 0.4827992523157545,\n",
       " 0.30024352421921163,\n",
       " 0.8061803245055488,\n",
       " 1.0,\n",
       " 0.3954922932910889,\n",
       " 0.5692392016414198,\n",
       " 0.38689983012194595,\n",
       " 0.30660939542312104,\n",
       " 0.4527044889852443,\n",
       " 0.0,\n",
       " 0.5320675800634052,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3219745573000369,\n",
       " 0.2849288443631641,\n",
       " 0.38489605564307827,\n",
       " 1.0,\n",
       " 0.48490605510532103,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6090674518895959,\n",
       " 0.6846473747010637,\n",
       " 0.20262811451638396,\n",
       " 0.5433997707943095,\n",
       " 0.27156819301279,\n",
       " 0.23211288530112462,\n",
       " 0.47053095022813185,\n",
       " 0.6546030491697581,\n",
       " 0.27835762804992337,\n",
       " 0.6345329421893272,\n",
       " 0.43446725981940015,\n",
       " 0.3343377583279381,\n",
       " 0.5282110223368404,\n",
       " 0.17000383561814136,\n",
       " 1.0,\n",
       " 0.28174091507232246,\n",
       " 0.15327597803967907,\n",
       " 0.612225687497798,\n",
       " 0.561032830377921,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.281210538298831,\n",
       " 0.6955833134064412,\n",
       " 0.0,\n",
       " 0.5578319367749439,\n",
       " 0.5126115623998788,\n",
       " 0.4847554558516347,\n",
       " 0.7414782298151074,\n",
       " 0.3198095010126204,\n",
       " 0.41758281096448013,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.23888680026525788,\n",
       " 0.6491986356465447,\n",
       " 0.507456205101669,\n",
       " 0.706252646381165,\n",
       " 0.26454021619621604,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.3116094017366853,\n",
       " 0.21778028257350546,\n",
       " 0.49842181522114093,\n",
       " 0.339343491867661,\n",
       " 0.0,\n",
       " 0.21417246666536183,\n",
       " 0.0,\n",
       " 0.6641864829585565,\n",
       " 0.0,\n",
       " 0.7490212482819971,\n",
       " 0.17789685420652068,\n",
       " 0.28047590710058656,\n",
       " 0.0,\n",
       " 0.7871044396387373,\n",
       " 0.6438473988211039,\n",
       " 1.0,\n",
       " 0.33843562296602225,\n",
       " 0.3533833230267531,\n",
       " 0.807032610563967,\n",
       " 0.5588422784285316,\n",
       " 0.4969164689014187,\n",
       " 0.7833695245316242,\n",
       " 0.3854231641887049,\n",
       " 0.7559309434504692,\n",
       " 0.3710636127102681,\n",
       " 0.7789302182346349,\n",
       " 0.40679735416043006,\n",
       " 0.5338574494469093,\n",
       " 0.7969778416007888,\n",
       " 0.7132516223755987,\n",
       " 0.28597803442794545,\n",
       " 0.8060913960633393,\n",
       " 0.7056023153131915,\n",
       " 0.8762331108625677,\n",
       " 0.49361310623234134,\n",
       " 0.37526988595055755,\n",
       " 0.5653672107971113,\n",
       " 0.31317502919419726,\n",
       " 0.7797241173852115,\n",
       " 0.728383104923581,\n",
       " 0.537655059552203,\n",
       " 0.7280614187301717,\n",
       " 0.7800506476123333,\n",
       " 0.6550293512154154,\n",
       " 0.5378856441332204,\n",
       " 0.7890555032959564,\n",
       " 0.7192559890928187,\n",
       " 0.5235364401514935,\n",
       " 0.5966791698745693,\n",
       " 0.48467775705538085,\n",
       " 0.4774653565240159,\n",
       " 0.5393352459653991,\n",
       " 0.5753294888719198,\n",
       " 0.8886253945320826,\n",
       " 0.45368663808037857,\n",
       " 0.5617383733323401,\n",
       " 0.4395704148583828,\n",
       " 0.568250422160132,\n",
       " 0.6147688194590971,\n",
       " 0.6482393773758324,\n",
       " 0.8303782399950751,\n",
       " 0.510087140758001,\n",
       " 0.6575630243888629,\n",
       " 0.5614198519827348,\n",
       " 0.7388704172821169,\n",
       " 0.4510901973346588,\n",
       " 0.6172748185872936,\n",
       " 0.6384728664008663,\n",
       " 0.7536961542330962,\n",
       " 0.7694748525311874,\n",
       " 0.6425034134735664,\n",
       " 0.43943333248812877,\n",
       " 0.45206930484700014,\n",
       " 0.6036274547435792,\n",
       " 0.5800379708919817,\n",
       " 0.6075474889378645,\n",
       " 0.47293191398133294,\n",
       " 0.6033352414489536,\n",
       " 0.8710641475457929,\n",
       " 0.4710984118834234,\n",
       " 0.5542331231354969,\n",
       " 0.7191415608695886,\n",
       " 0.3410871827572157,\n",
       " 0.7386519015711646,\n",
       " 0.6307458151677343,\n",
       " 0.4723052628959741,\n",
       " 0.5677336572409835,\n",
       " 0.5363224977280127,\n",
       " 0.6316583298793843,\n",
       " 0.5962024113137894,\n",
       " 0.6792949324840446,\n",
       " 0.5007135556817937,\n",
       " 0.6480747898496585,\n",
       " 0.2673073284349996,\n",
       " 0.6032449647223925,\n",
       " 0.18796309414600543,\n",
       " 0.7821756390339755,\n",
       " 0.5526488375164476,\n",
       " 0.565435157154873,\n",
       " 0.5323569163048228,\n",
       " 0.6165617133797364,\n",
       " 0.5288543425424144,\n",
       " 0.612552821716356,\n",
       " 0.40981021123878547,\n",
       " 0.8152230679662728,\n",
       " 0.5349892172155912,\n",
       " 0.7227741709423376,\n",
       " 0.44535307964767035,\n",
       " 0.7335916608467525,\n",
       " 0.5219136535289064,\n",
       " 0.5586310799918579,\n",
       " 0.5985145021081398,\n",
       " 0.5382221617352677,\n",
       " 0.5880451250267861,\n",
       " 0.34701572024027594,\n",
       " 0.7615972746341745,\n",
       " 0.6217087120235364,\n",
       " 0.6562729790958813,\n",
       " 0.564568429578883,\n",
       " 0.7467645246032611,\n",
       " 0.5088259536847448,\n",
       " 0.38737964351750953,\n",
       " 0.5061355183858008,\n",
       " 0.5462005583632086,\n",
       " 0.6059840012048179,\n",
       " 0.641149740248289,\n",
       " 0.2592717713379644,\n",
       " 0.7107322732626838,\n",
       " 0.8516988474931363,\n",
       " 0.44198994732909314,\n",
       " 0.5458936951931888,\n",
       " 0.5033306169957211,\n",
       " 0.2515293168803814,\n",
       " 0.5947836969882396,\n",
       " 0.5748299095373731,\n",
       " 0.6082228431227119,\n",
       " 0.44248204691962334,\n",
       " 0.525599280438097,\n",
       " ...]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baselines[\"TF-IDF Cosine with Stop Words\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline #8: Doc2vec embeddings + cosine similarity (Doc2vec + cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This baseline constructs document embeddings using doc2vec and then applies cosine similarity to measure each sentence pair's similarity (for an introduction to the cosine similarity metric, see [Background on Cosine Similarity](#What-is-Cosine-Similarity?))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Doc2Vec?\n",
    "\n",
    "Doc2vec is an extension of word2vec which produces embeddings of a document. Note that \"document\" refers to some larger chunk of multiple tokens/words. In our case, our documents will actually be individual setntences. The algorithm not only exploits the idea of context words (like in word2vec), but also incorporates the context of the document. There are again two model architectures that parallel those of word2vec: Paragraph Vectors Distributed Memory (PV-DM) and Paragraph Vectors Distributed Bag-of-Words (PV-DBOW). PV-DM randomly samples consecutive words in a paragraph and predicts a center word by utilizing the context words and the paragraph id. PV-DBOW takes a paragraph id and uses it to predict words in the context. \n",
    "\n",
    "See [tutorial #1](https://kanoki.org/2019/03/07/sentence-similarity-in-python-using-doc2vec/) or [tutorial #2](https://gab41.lab41.org/doc2vec-to-assess-semantic-similarity-in-source-code-667acb3e62d7) for more information and an example of using Doc2vec for sentence similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc2vec requires unique ids for each sentence, so we'll iterate through making a dictionary of sentence to its id\n",
    "id_dict = {}\n",
    "n = 0\n",
    "\n",
    "\n",
    "def assign_id(row):\n",
    "    global n\n",
    "    if row not in id_dict:\n",
    "        id_dict[row] = n\n",
    "        n += 1\n",
    "    return id_dict[row]\n",
    "\n",
    "\n",
    "sts_test_stop[\"qid1\"] = sts_test_stop[\"sentence1\"].apply(assign_id)\n",
    "sts_test_stop[\"qid2\"] = sts_test_stop[\"sentence2\"].apply(assign_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec_cosine(df, rm_stopwords=False):\n",
    "    \"\"\"Calculate cosine similarity between each sentence pair using Doc2Vec embeddings\n",
    "    \n",
    "    Args:\n",
    "        df (pandas dataframe): dataframe as provided by the nlp_utils\n",
    "        stop_words (bool): whether to use stop words (True) or remove them (False)\n",
    "    \n",
    "    Returns:\n",
    "        list: predicted values for sentence similarity of test set examples\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    if rm_stopwords:\n",
    "        tokenized_sentences = zip(\n",
    "            df[\"sentence1_tokens_stop\"], df[\"sentence2_tokens_stop\"]\n",
    "        )\n",
    "    else:\n",
    "        tokenized_sentences = zip(df[\"sentence1_tokens\"], df[\"sentence2_tokens\"])\n",
    "\n",
    "    labeled_questions = []\n",
    "    sentences = list(tokenized_sentences)\n",
    "    # doc2vec requires data as Tagged Documents with the tokenized sentence and the sentence id\n",
    "    for i in df.index:\n",
    "        labeled_questions.append(\n",
    "            TaggedDocument(sentences[i][0], df[df.index == i].qid1)\n",
    "        )\n",
    "        labeled_questions.append(\n",
    "            TaggedDocument(sentences[i][1], df[df.index == i].qid2)\n",
    "        )\n",
    "\n",
    "    # instantiate Doc2Vec model\n",
    "    model = Doc2Vec(\n",
    "        labeled_questions, dm=1, min_count=1, window=5, vector_size=500, epochs=30\n",
    "    )\n",
    "\n",
    "    # Train our model for 20 epochs\n",
    "    for epoch in range(20):\n",
    "        model.train(\n",
    "            labeled_questions, epochs=model.epochs, total_examples=model.corpus_count\n",
    "        )\n",
    "\n",
    "    # Get similarity between all sentence pairs in our dataset\n",
    "    for (sentence1, sentence2) in sentences:\n",
    "        if len(sentence1) == 0 or len(sentence2) == 0:\n",
    "            predictions.append(0)\n",
    "            continue\n",
    "        score = model.wv.n_similarity(sentence1, sentence2)\n",
    "        predictions.append(score)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines[\"Doc2vec Cosine\"] = doc2vec_cosine(sts_test_stop, rm_stopwords=True)\n",
    "baselines[\"Doc2vec Cosine with Stop Words\"] = doc2vec_cosine(sts_test_stop, rm_stopwords=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation metric is Pearson correlation ($\\rho$) which is a measure of the linear correlation between two variables. The formula for calculating Pearson correlation is as follows:  \n",
    "\n",
    "$$\\rho_{X,Y} = \\frac{E[(X-\\mu_X)(Y-\\mu_Y)]}{\\sigma_X \\sigma_Y}$$\n",
    "\n",
    "This metric takes a value in [-1,1] where -1 represents a perfect negative correlation, 1 represents a perfect positive correlation, and 0 represents no correlation. We utilize the Pearson correlation metric as this is the metric that [SentEval](http://nlpprogress.com/english/semantic_textual_similarity.html), a widely-used evaluation toolkit for evaluation sentence representations, uses for the STS Benchmark dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_correlation(df, prediction):\n",
    "    \"\"\"Calculate the Pearson correlation between two vectors\n",
    "    \n",
    "    Args:\n",
    "        df (pandas dataframe): dataframe of sentences and their similarity scores\n",
    "        prediction (list): predicted similarity scores for each value in test set\n",
    "        \n",
    "    Returns:\n",
    "        float: pearson correlation value between the actual and predicted score lists\n",
    "    \"\"\"\n",
    "    pearson_correlation = scipy.stats.pearsonr(prediction, list(df[\"score\"]))[0]\n",
    "    return pearson_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Cosine 0.7034695168223283\n",
      "TF-IDF Cosine with Stop Words 0.6683811410442564\n"
     ]
    }
   ],
   "source": [
    "# Get metrics on predictions from all models\n",
    "for model in baselines:\n",
    "    print(model, pearson_correlation(sts_test_stop, baselines[model]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We investigate our 8 models with and without stop words (16 different results today). The results show that TF-IDF bag-of-words document embeddings (without stop words) combined with the cosine similarity performs the best, with a Pearson correlation of 0.7034. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
