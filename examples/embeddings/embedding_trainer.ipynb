{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than use pre-trained embeddings (as we did in the baseline_deep_dive notebook), we can train word embeddings using our own dataset. In this notebook, we demonstrate the training process for producing word embeddings using the word2vec, GloVe, and fastText models. We'll utilize the STS Benchmark dataset for this task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Data Loading and Preprocessing](#Load-and-Preprocess-Data)\n",
    "* [Word2Vec](#Word2Vec)\n",
    "* [fastText](#fastText)\n",
    "* [GloVe](#GloVe)\n",
    "* [Concluding Remarks](#Concluding-Remarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import sys\n",
    "# Set the environment path\n",
    "sys.path.append(\"../../\") \n",
    "import os\n",
    "from utils_nlp.dataset.preprocess import (\n",
    "    to_lowercase,\n",
    "    to_spacy_tokens,\n",
    "    rm_spacy_stopwords,\n",
    ")\n",
    "from utils_nlp.dataset import stsbenchmark\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.fasttext import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path for where your datasets are located\n",
    "BASE_DATA_PATH = \"../../data\" \n",
    "# Location to save embeddings\n",
    "SAVE_FILES_PATH = BASE_DATA_PATH + \"/trained_word_embeddings/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(SAVE_FILES_PATH):\n",
    "    os.makedirs(SAVE_FILES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a pandas dataframe for the training set\n",
    "sts_train = stsbenchmark.load_pandas_df(BASE_DATA_PATH, file_split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training set preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all text to lowercase\n",
    "df_low = to_lowercase(sts_train)  \n",
    "# Tokenize text\n",
    "sts_tokenize = to_spacy_tokens(df_low) \n",
    "# Tokenize with removal of stopwords\n",
    "sts_train_stop = rm_spacy_stopwords(sts_tokenize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Append together the two sentence columns to get a list of all tokenized sentences.\n",
    "all_sentences =  sts_train_stop[[\"sentence1_tokens_rm_stopwords\", \"sentence2_tokens_rm_stopwords\"]]\n",
    "sentences = all_sentences.values.flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['plane', 'taking', '.'],\n",
       " ['air', 'plane', 'taking', '.'],\n",
       " ['man', 'playing', 'large', 'flute', '.'],\n",
       " ['man', 'playing', 'flute', '.'],\n",
       " ['man', 'spreading', 'shreded', 'cheese', 'pizza', '.'],\n",
       " ['man', 'spreading', 'shredded', 'cheese', 'uncooked', 'pizza', '.'],\n",
       " ['men', 'playing', 'chess', '.'],\n",
       " ['men', 'playing', 'chess', '.'],\n",
       " ['man', 'playing', 'cello', '.'],\n",
       " ['man', 'seated', 'playing', 'cello', '.']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec is a predictive model for learning word embeddings from text. Word embeddings are learned such that words that share common contexts in the corpus will be close together in the vector space. There are two different model architectures that can be used to produce word2vec embeddings: continuous bag-of-words (CBOW) or continuous skip-gram. The former uses a window of surrounding words (the \"context\") to predict the current word and the latter uses the current word to predict the surrounding context words. See this [tutorial](https://www.guru99.com/word-embedding-word2vec.html#3) on word2vec for more detailed background on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gensim Word2Vec model has many different parameters (see [here](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec)) but the ones that are useful to know about are:  \n",
    "- size: length of the word embedding/vector (defaults to 100)\n",
    "- window: maximum distance between the word being predicted and the current word (defaults to 5)\n",
    "- min_count: ignores all words that have a frequency lower than this value (defaults to 5)\n",
    "- workers: number of worker threads used to train the model (defaults to 3)\n",
    "- sg: training algorithm; 1 for skip-gram and 0 for CBOW (defaults to 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=3, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained we can:\n",
    "\n",
    "1. Query for the word embeddings of a given word. \n",
    "2. Inspect the model vocabulary\n",
    "3. Save the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for apple: [-1.45744249e-01 -4.23673481e-01  3.25938733e-03  2.16118336e-01\n",
      " -3.38810831e-02  4.36680727e-02 -9.32965130e-02 -1.30193695e-01\n",
      "  1.22479253e-01 -5.57641592e-03  1.15231648e-01 -1.90644771e-01\n",
      "  1.36980727e-01  1.54277921e-01  3.02371562e-01 -1.50472924e-01\n",
      "  1.22283474e-01  1.66438103e-01 -4.53472175e-02 -7.50410557e-02\n",
      "  9.35729593e-02  2.84669697e-02 -1.56790614e-01  2.13695884e-01\n",
      "  3.59700769e-02 -2.28801727e-01 -6.65102080e-02 -1.00866072e-01\n",
      "  1.73690766e-02 -6.05353080e-02  6.91714063e-02 -1.40613124e-01\n",
      " -1.55061241e-02  3.15357596e-02  1.05547914e-02  8.11263919e-02\n",
      "  8.59601721e-02 -1.93002746e-01  5.66715710e-02  4.07571755e-02\n",
      " -5.99270537e-02  8.00862685e-02 -7.79552236e-02 -1.84332090e-03\n",
      " -3.64068709e-02  9.83870868e-03  2.55715717e-02 -6.73236232e-03\n",
      " -1.00992797e-02  2.55039651e-02  7.85838366e-02  1.66635379e-01\n",
      "  5.47658615e-02  1.30082041e-01  1.10447399e-01  3.11631355e-02\n",
      " -7.80537724e-02  5.18930964e-02  1.29589550e-02  4.16627787e-02\n",
      " -1.24468409e-01  3.77246141e-02 -1.36392683e-01  3.56191322e-02\n",
      "  1.69469059e-01  3.05777900e-02 -1.10528849e-01 -1.67536646e-01\n",
      "  5.50368614e-03 -1.53962612e-01 -1.37974005e-02 -1.76773630e-02\n",
      "  1.97921172e-01  6.10002689e-02  7.60239959e-02  1.03102773e-01\n",
      "  5.69388792e-02  1.14771552e-01 -2.30673775e-02  8.62826407e-02\n",
      " -4.97789308e-02  1.38558373e-01 -1.61710054e-01 -2.54187167e-01\n",
      " -2.03636363e-02 -2.23468933e-02  5.76032698e-02  5.81657439e-02\n",
      " -4.40338664e-02 -3.12582515e-02  4.77962138e-04  1.27670109e-01\n",
      " -1.19244941e-01 -7.19835982e-02  6.29036576e-02  2.19937578e-01\n",
      "  2.86382070e-04  1.14967981e-02 -8.17993656e-02  2.69429274e-02]\n",
      "\n",
      "First 30 vocabulary words: ['plane', 'taking', '.', 'air', 'man', 'playing', 'large', 'flute', 'spreading', 'cheese', 'pizza', 'men', 'seated', 'fighting', 'smoking', 'piano', 'guitar', 'singing', 'woman', 'person']\n"
     ]
    }
   ],
   "source": [
    "# 1. Let's see the word embedding for \"apple\" by accessing the \"wv\" attribute and passing in \"apple\" as the key.\n",
    "print(\"Embedding for apple:\", word2vec_model.wv[\"apple\"])\n",
    "\n",
    "# 2. Inspect the model vocabulary by accessing keys of the \"wv.vocab\" attribute. We'll print the first 20 words.\n",
    "print(\"\\nFirst 30 vocabulary words:\", list(word2vec_model.wv.vocab)[:20])\n",
    "\n",
    "# 3. Save the word embeddings. We can save as binary format (to save space) or ASCII format.\n",
    "word2vec_model.wv.save_word2vec_format(SAVE_FILES_PATH+\"word2vec_model\", binary=True)  # binary format\n",
    "word2vec_model.wv.save_word2vec_format(SAVE_FILES_PATH+\"word2vec_model\", binary=False)  # ASCII format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastText is an unsupervised algorithm created by Facebook Research for efficiently learning word embeddings. fastText is significantly different than word2vec or GloVe in that these two algorithms treat each word as the smallest possible unit to find an embedding for. Conversely, fastText assumes that words are formed by an n-gram of characters (i.e. 2-grams of the word \"language\" would be {la, an, ng, gu, ua, ag, ge}). The embedding for a word is then composed of the sum of these character n-grams. This has advantages when finding word embeddings for rare words and words not present in the dictionary, as these words can still be broken down into character n-grams. Typically, for smaller datasets, fastText performs better than word2vec or GloVe. See this [tutorial](https://fasttext.cc/docs/en/unsupervised-tutorial.html) on fastText for more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gensim fastText model has many different parameters (see [here](https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText)) but the ones that are useful to know about are:  \n",
    "- size: length of the word embedding/vector (defaults to 100)\n",
    "- window: maximum distance between the word being predicted and the current word (defaults to 5)\n",
    "- min_count: ignores all words that have a frequency lower than this value (defaults to 5)\n",
    "- workers: number of worker threads used to train the model (defaults to 3)\n",
    "- sg: training algorithm- 1 for skip-gram and 0 for CBOW (defaults to 0)\n",
    "- iter: number of epochs (defaults to 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_model = FastText(size=100, window=5, min_count=5, sentences=sentences, iter=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can utilize the same attributes as we saw above for word2vec due to them both originating from the gensim package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for apple: [ 0.28720722  0.09591733  0.54433066 -0.05831062 -0.16943751  0.3623266\n",
      " -0.15930647 -0.11854805 -0.51492     0.34766328  0.14267923  0.05289214\n",
      "  0.50301915  0.36670431 -0.0856811   0.40855268 -0.42051238 -0.42935574\n",
      " -0.00590014  0.1055185   0.19841222  0.28653464  0.02694756 -0.2998636\n",
      "  0.2037611  -0.09531905 -0.18712749 -0.0126017   0.37071258 -0.10778084\n",
      "  0.05781018 -0.28327507  0.36217982  0.19254316  0.39235938 -0.25031894\n",
      " -0.05957209  0.88860816  0.07133473  0.08105652 -0.2374883  -0.00684688\n",
      " -0.2799134   0.03850232 -0.03038069  0.09097645 -0.21751858 -0.34727782\n",
      " -0.2102714   0.06907686 -0.3250014   0.14360848  0.06191867  0.30928102\n",
      "  0.00916505 -0.2980404   0.08639123  0.43495005  0.2583049  -0.05802987\n",
      "  0.25165558 -0.05238454  0.21494739  0.16128843  0.23630676 -0.60110664\n",
      "  0.41299316  0.09353159 -0.03104531 -0.17996807  0.11146976 -0.0121649\n",
      "  0.06300905  0.30792916  0.06998671 -0.1877395   0.02814171  0.10962378\n",
      " -0.05561887  0.01427535  0.49409914  0.26800776  0.0756273   0.04907822\n",
      " -0.48029277 -0.24623543  0.04248046  0.27225688  0.506395    0.37661335\n",
      " -0.05463752 -0.11666346  0.3165078  -0.2695211  -0.09770845 -0.04777316\n",
      "  0.41947618 -0.05776473  0.2840346  -0.175891  ]\n",
      "\n",
      "First 30 vocabulary words: ['plane', 'taking', '.', 'air', 'man', 'playing', 'large', 'flute', 'spreading', 'cheese', 'pizza', 'men', 'seated', 'fighting', 'smoking', 'piano', 'guitar', 'singing', 'woman', 'person']\n"
     ]
    }
   ],
   "source": [
    "# 1. Let's see the word embedding for \"apple\" by accessing the \"wv\" attribute and passing in \"apple\" as the key.\n",
    "print(\"Embedding for apple:\", fastText_model.wv[\"apple\"])\n",
    "\n",
    "# 2. Inspect the model vocabulary by accessing keys of the \"wv.vocab\" attribute. We'll print the first 20 words.\n",
    "print(\"\\nFirst 30 vocabulary words:\", list(fastText_model.wv.vocab)[:20])\n",
    "\n",
    "# 3. Save the word embeddings. We can save as binary format (to save space) or ASCII format.\n",
    "fastText_model.wv.save_word2vec_format(SAVE_FILES_PATH+\"fastText_model\", binary=True)  # binary format\n",
    "fastText_model.wv.save_word2vec_format(SAVE_FILES_PATH+\"fastText_model\", binary=False)  # ASCII format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe is an unsupervised algorithm for obtaining word embeddings created by the Stanford NLP group. Training occurs on word-word co-occurrence statistics with the objective of learning word embeddings such that the dot product of two words' embeddings is equal to the words' probability of co-occurrence. See this [tutorial](https://nlp.stanford.edu/projects/glove/) on GloVe for more detailed background on the model. \n",
    "\n",
    "Gensim doesn't have an implementation of the GloVe model, so we suggest getting the code directly from the Stanford NLP [repo](https://github.com/stanfordnlp/GloVe). Run the following commands to clone the repo and then make. Clone the repo in the same location as this notebook! Otherwise, the paths below will need to be modified.  \n",
    "\n",
    "    git clone http://github.com/stanfordnlp/glove    \n",
    "    cd glove && make  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train GloVe vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training GloVe embeddings requires some data prep and then 4 steps (also documented in the original Stanford NLP repo [here](https://github.com/stanfordnlp/GloVe/tree/master/src))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0: Prepare Data**\n",
    "   \n",
    "In order to train our GloVe vectors, we first need to save our corpus as a text file with all words separated by 1+ spaces or tabs. Each document/sentence is separated by a new line character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our corpus as tokens delimited by spaces with new line characters in between sentences.\n",
    "with open(BASE_DATA_PATH+'/clean/stsbenchmark/training-corpus-cleaned.txt', 'w', encoding='utf8') as file:\n",
    "    for sent in sentences:\n",
    "        file.write(\" \".join(sent) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Build Vocabulary**\n",
    "\n",
    "Run the vocab_count executable. There are 3 optional parameters:\n",
    "1. min-count: lower limit on how many times a word must appear in dataset. Otherwise the word is discarded from our vocabulary.\n",
    "2. max-vocab: upper bound on the number of vocabulary words to keep\n",
    "3. verbose: 0, 1, or 2 (default)\n",
    "\n",
    "Then provide the path to the text file we created in Step 0 followed by a file path that we'll save the vocabulary to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BUILDING VOCABULARY\n",
      "Processed 0 tokens.Processed 84997 tokens.\n",
      "Counted 11716 unique words.\n",
      "Truncating vocabulary at min count 5.\n",
      "Using vocabulary of size 2943.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!\"glove/build/vocab_count\" -min-count 5 -verbose 2 <\"../../data/clean/stsbenchmark/training-corpus-cleaned.txt\"> \"../../data/trained_word_embeddings/vocab.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Construct Word Co-occurrence Statistics**\n",
    "\n",
    "Run the cooccur executable. There are many optional parameters, but we list the top ones here:\n",
    "1. symmetric: 0 for only looking at left context, 1 (default) for looking at both left and right context\n",
    "2. window-size: number of context words to use (default 15)\n",
    "3. verbose: 0, 1, or 2 (default)\n",
    "4. vocab-file: path/name of the vocabulary file created in Step 1\n",
    "5. memory: soft limit for memory consumption, default 4\n",
    "6. max-product: limit the size of dense co-occurrence array by specifying the max product (integer) of the frequency counts of the two co-occurring words\n",
    "\n",
    "Then provide the path to the text file we created in Step 0 followed by a file path that we'll save the co-occurrences to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COUNTING COOCCURRENCES\n",
      "window size: 15\n",
      "context: symmetric\n",
      "max product: 13752509\n",
      "overflow length: 38028356\n",
      "Reading vocab from file \"../../data/trained_word_embeddings/vocab.txt\"...loaded 2943 words.\n",
      "Building lookup table...table contains 8661250 elements.\n",
      "Processing token: 0Processed 84997 tokens.\n",
      "Writing cooccurrences to disk......2 files in total.\n",
      "Merging cooccurrence files: processed 0 lines.0 lines.100000 lines.Merging cooccurrence files: processed 187717 lines.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!\"glove/build/cooccur\" -memory 4 -vocab-file \"../../data/trained_word_embeddings/vocab.txt\" -verbose 2 -window-size 15 <\"../../data/clean/stsbenchmark/training-corpus-cleaned.txt\"> \"../../data/trained_word_embeddings/cooccurrence.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Shuffle the Co-occurrences**\n",
    "\n",
    "Run the shuffle executable. The parameters are as follows:\n",
    "1. verbose: 0, 1, or 2 (default)\n",
    "2. memory: soft limit for memory consumption, default 4\n",
    "3. array-size: limit to the length of the buffer which stores chunks of data to shuffle before writing to disk\n",
    "\n",
    "Then provide the path to the co-occurrence file we created in Step 2 followed by a file path that we'll save the shuffled co-occurrences to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SHUFFLING COOCCURRENCES\n",
      "array size: 255013683\n",
      "Shuffling by chunks: processed 0 lines.processed 187717 lines.\n",
      "Wrote 1 temporary file(s).\n",
      "Merging temp files: processed 0 lines.187717 lines.Merging temp files: processed 187717 lines.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!\"glove/build/shuffle\" -memory 4 -verbose 2 <\"../../data/trained_word_embeddings/cooccurrence.bin\"> \"../../data/trained_word_embeddings/cooccurrence.shuf.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Train GloVe model**\n",
    "\n",
    "Run the glove executable. There are many parameter options, but the top ones are listed below:\n",
    "1. verbose: 0, 1, or 2 (default)\n",
    "2. vector-size: dimension of word embeddings (50 is default)\n",
    "3. threads: number threads, default 8\n",
    "4. iter: number of iterations, default 25\n",
    "5. eta: learning rate, default 0.05\n",
    "6. binary: whether to save binary format (0: text = default, 1: binary, 2: both)\n",
    "7. x-max: cutoff for weighting function, default is 100\n",
    "8. vocab-file: file containing vocabulary as produced in Step 1\n",
    "9. save-file: filename to save vectors to \n",
    "10. input-file: filename with co-occurrences as returned from Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL\n",
      "Read 187717 lines.\n",
      "Initializing parameters...done.\n",
      "vector size: 50\n",
      "vocab size: 2943\n",
      "x_max: 10.000000\n",
      "alpha: 0.750000\n",
      "05/06/19 - 08:43.06PM, iter: 001, cost: 0.078361\n",
      "05/06/19 - 08:43.06PM, iter: 002, cost: 0.072087\n",
      "05/06/19 - 08:43.06PM, iter: 003, cost: 0.070096\n",
      "05/06/19 - 08:43.06PM, iter: 004, cost: 0.067155\n",
      "05/06/19 - 08:43.06PM, iter: 005, cost: 0.063527\n",
      "05/06/19 - 08:43.06PM, iter: 006, cost: 0.060744\n",
      "05/06/19 - 08:43.06PM, iter: 007, cost: 0.058127\n",
      "05/06/19 - 08:43.06PM, iter: 008, cost: 0.056086\n",
      "05/06/19 - 08:43.06PM, iter: 009, cost: 0.054027\n",
      "05/06/19 - 08:43.06PM, iter: 010, cost: 0.051815\n",
      "05/06/19 - 08:43.06PM, iter: 011, cost: 0.049561\n",
      "05/06/19 - 08:43.06PM, iter: 012, cost: 0.047386\n",
      "05/06/19 - 08:43.06PM, iter: 013, cost: 0.045234\n",
      "05/06/19 - 08:43.06PM, iter: 014, cost: 0.043166\n",
      "05/06/19 - 08:43.06PM, iter: 015, cost: 0.041163\n"
     ]
    }
   ],
   "source": [
    "!\"glove/build/glove\" -save-file \"../../data/trained_word_embeddings/GloVe_vectors\" -threads 8 -input-file \\\n",
    "\"../../data/trained_word_embeddings/cooccurrence.shuf.bin\" -x-max 10 -iter 15 -vector-size 50 -binary 2 \\\n",
    "-vocab-file \"../../data/trained_word_embeddings/vocab.txt\" -verbose 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we did above for the word2vec and fastText models, let's now inspect our word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the saved word vectors.\n",
    "glove_wv = {}\n",
    "with open(\"../../data/trained_word_embeddings/GloVe_vectors.txt\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        split_line = line.split(\" \")\n",
    "        glove_wv[split_line[0]] = [float(i) for i in split_line[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for apple: [0.106778, -0.054364, 0.071933, 0.107653, 0.029326, -0.112638, 0.026381, 0.022774, -0.05421, 0.074924, -0.043991, 0.140131, 0.034497, 0.072747, 0.051646, -0.082894, 0.064107, -0.063781, 0.028841, -0.001721, 0.132145, 0.133201, 0.066409, 0.121694, -0.232013, 0.016408, 0.020115, 0.126312, -0.136232, -0.084801, 0.048155, -0.072803, -0.100928, 0.066932, 0.002329, -0.022856, 0.042825, -0.039268, -0.030836, -0.071207, 0.060238, -0.032366, 0.021732, -0.120279, 0.132316, 0.069412, -0.067842, 0.04007, -0.177785, -0.001198]\n",
      "\n",
      "First 30 vocabulary words: ['.', ',', 'man', '-', 'woman', \"'\", 'said', 'dog', '\"', 'playing', ':', 'white', 'black', '$', 'killed', 'percent', 'new', 'syria', 'people', 'china']\n"
     ]
    }
   ],
   "source": [
    "# 1. Let's see the word embedding for \"apple\" by passing in \"apple\" as the key.\n",
    "print(\"Embedding for apple:\", glove_wv[\"apple\"])\n",
    "\n",
    "# 2. Inspect the model vocabulary by accessing keys of the \"wv.vocab\" attribute. We'll print the first 20 words.\n",
    "print(\"\\nFirst 30 vocabulary words:\", list(glove_wv.keys())[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concluding Remarks\n",
    "\n",
    "In this notebook we have shown how to train word2vec, GloVe, and fastText word embeddings on the STS Benchmark dataset. FastText is typically regarded as the best baseline for word embeddings (see [blog](https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a)) and is a good place to start when generating word embeddings. Now that we generated word embeddings on our dataset, we could also repeat the baseline_deep_dive notebook using these embeddings (versus the pre-trained ones from the internet). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
